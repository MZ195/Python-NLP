{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf2db69-ea1b-4639-bf93-aa2ab6dcbc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown, movie_reviews\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c62a0-2a6a-4c91-8672-40e5208763ce",
   "metadata": {},
   "source": [
    "## Skip gram vs CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6652fb16-2108-4c0c-aeb2-2931f1a97adc",
   "metadata": {},
   "source": [
    "Simply put, the CBOW model learns the embedding by predicting the current word based on its context. The skip-gram model learns by predicting the surrounding words given a current word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2b692b-a3f9-4231-ba25-ec5b610acb41",
   "metadata": {},
   "source": [
    "![](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/08/Word2Vec-Training-Models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16156008-8b26-4b74-83e7-54c9536d4988",
   "metadata": {},
   "source": [
    "#### Using Skipgram method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "353abccd-93b1-4f59-9d13-e3d1489fe091",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = word2vec.Text8Corpus('./data/text8')\n",
    "model_sg = word2vec.Word2Vec(sentences, vector_size=100,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f0c61e-c5af-4246-9f1d-232ff6383476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dividends', 0.7778456807136536),\n",
       " ('credit', 0.759507954120636),\n",
       " ('accrued', 0.7531089186668396),\n",
       " ('repayment', 0.7517944574356079),\n",
       " ('repay', 0.7475109100341797),\n",
       " ('lending', 0.7417505383491516),\n",
       " ('specie', 0.7380414605140686),\n",
       " ('payments', 0.7368233799934387),\n",
       " ('profits', 0.735568106174469),\n",
       " ('cheques', 0.7277211546897888)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sg.wv.most_similar(\"money\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "022984cf-e25d-40ac-91df-5d98ed562d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('highness', 0.7714400291442871),\n",
       " ('elizabeth', 0.7651525139808655),\n",
       " ('prince', 0.7617310285568237),\n",
       " ('consort', 0.7615090012550354),\n",
       " ('regnant', 0.7459958791732788),\n",
       " ('isabella', 0.7270307540893555),\n",
       " ('victoria', 0.7214810252189636),\n",
       " ('margrethe', 0.7168237566947937),\n",
       " ('hrh', 0.7142952680587769),\n",
       " ('princess', 0.7080675959587097)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sg.wv.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "238014ec-67dd-493b-98ce-e901a23b4593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('daughter', 0.6312880516052246),\n",
       " ('throne', 0.6281335353851318),\n",
       " ('matilda', 0.6162846684455872),\n",
       " ('jagiellon', 0.6146883368492126),\n",
       " ('queen', 0.6145486831665039)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sg.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b25ef384-bff7-405b-8d14-068d6508366e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sinuous', 0.905532717704773),\n",
       " ('wadis', 0.9010744690895081),\n",
       " ('basaltic', 0.8984476923942566),\n",
       " ('watercourses', 0.8929212093353271),\n",
       " ('troughs', 0.890213131904602),\n",
       " ('sinkholes', 0.8894224166870117),\n",
       " ('uplifted', 0.8881569504737854),\n",
       " ('featureless', 0.8815904259681702),\n",
       " ('undulating', 0.8756296038627625),\n",
       " ('outwash', 0.8742260932922363)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sg.wv.most_similar(\"meandering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e39e4d-c1ae-4227-8ef6-3a569e15ea8c",
   "metadata": {},
   "source": [
    "#### Learning:\n",
    "In CBOW the vectors from the context words are averaged before predicting the center word. In skip-gram there is no averaging of embedding vectors. It seems like the model can learn better representations for the rare words when their vectors are not averaged with the other context words in the process of making the predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d612c2-caf2-42d1-ab3f-164082af3a53",
   "metadata": {},
   "source": [
    "## Word vectors trained on different contexts\n",
    " - We'll load different corpora, from different contexts and see how the embeddings vary\n",
    " - The text8 corpus is wikipedia pages, while Brown corpus is from 15 different topics, and movie reviews are from IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b740390e-9ce8-4bff-b558-725a679e0618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/mz195/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e492d4c0-5bc1-4453-835b-71cadacde186",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_brown = Word2Vec(brown.sents(), sg=1)\n",
    "model_movie = Word2Vec(movie_reviews.sents(), sg=1, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9173910c-f798-49f1-9e11-98ba48fab198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dividends', 0.7778456807136536),\n",
       " ('credit', 0.759507954120636),\n",
       " ('accrued', 0.7531089186668396),\n",
       " ('repayment', 0.7517944574356079),\n",
       " ('repay', 0.7475109100341797)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sg.wv.most_similar('money', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c1f4d55-8ec3-4c7d-8f6b-8f60d10ac57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('care', 0.8328453898429871),\n",
       " ('job', 0.8285400867462158),\n",
       " ('friendship', 0.8189941048622131),\n",
       " ('risk', 0.8062323927879333),\n",
       " ('joy', 0.8045491576194763)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_brown.wv.most_similar('money', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a155bb3-935d-4c17-a0ab-5023d7800579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cash', 0.7353739738464355),\n",
       " ('paid', 0.7042819857597351),\n",
       " ('ransom', 0.6965006589889526),\n",
       " ('record', 0.6914075613021851),\n",
       " ('risk', 0.6876891851425171)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_movie.wv.most_similar('money', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee9299c-3e6e-49b4-af7d-fe82865afe65",
   "metadata": {},
   "source": [
    "## Using pre-trained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946691b6-658a-4f7b-ab12-1f2bf699b679",
   "metadata": {},
   "source": [
    "### A quick note on Glove:\n",
    "   - Developed by Stanford by training on 6 Billion tokens\n",
    "   - Objective is slightly different\n",
    "   - End result very similar to Google's word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d37db07-17e6-4432-ae70-f6489c051b32",
   "metadata": {},
   "source": [
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f14c75-4cb3-458a-8998-6334098e0dcb",
   "metadata": {},
   "source": [
    "- We'll use the 100D vectors for this example.\n",
    "- The trained vectors are available in a text file\n",
    "- The format is slightly different from that of word2vec, necessitating the use of a utility to format accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47df3a25-6e02-4352-a427-9c7928f106b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-7498a3aef3af>:3: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_input_file, word2vec_output_file)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_input_file = './data/glove.6B/glove.6B.100d.txt'\n",
    "word2vec_output_file = './data/glove.6B/glove.6B.100d.w2vformat.txt'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a29bd646-8b3c-44f5-b0b4-620b6eda9ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = KeyedVectors.load_word2vec_format(\"./data/glove.6B/glove.6B.100d.w2vformat.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a918ae1e-c72a-4fea-a402-ab36be6a2727",
   "metadata": {},
   "source": [
    "#### Now you can use all the methods you used with word2vec models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "380850fb-a895-4f85-adab-4d11421c8ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prince', 0.7682328820228577),\n",
       " ('queen', 0.7507690787315369),\n",
       " ('son', 0.7020888328552246),\n",
       " ('brother', 0.6985775232315063),\n",
       " ('monarch', 0.6977890729904175),\n",
       " ('throne', 0.6919989585876465),\n",
       " ('kingdom', 0.6811409592628479),\n",
       " ('father', 0.6802029013633728),\n",
       " ('emperor', 0.6712858080863953),\n",
       " ('ii', 0.6676074266433716)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.most_similar(\"king\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc464d88-3876-4f4f-b8fc-654bbad4bd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7698540687561035),\n",
       " ('monarch', 0.6843381524085999),\n",
       " ('throne', 0.6755736470222473),\n",
       " ('daughter', 0.6594556570053101),\n",
       " ('princess', 0.6520534157752991)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3dd3f01-db6a-4792-aa63-5096964dae8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('heroine', 0.732498049736023),\n",
       " ('heroes', 0.6356217861175537),\n",
       " ('icon', 0.6185224056243896),\n",
       " ('beloved', 0.6136684417724609),\n",
       " ('herself', 0.5904076099395752)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.most_similar(positive=['woman', 'hero'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f531e9f1-ef4e-4b18-a5e4-32daf58af5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
