{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATIS Flight Reservations - Information Extraction\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "Table of Contents:\n",
    "\n",
    "1. Understanding the Data\n",
    "2. Information Extraction \n",
    "    - Pipeline for Information Extraction Systems\n",
    "    - Named Entity Recognition (NER)\n",
    "3. Models for Entity Recognition\n",
    "    - Rule-based models\n",
    "        - Regular Expression Based Rules (ex)\n",
    "        - Chunking \n",
    "    - Probabilistic models\n",
    "        - Unigram and Bigram models\n",
    "        - Naive Bayes Classifier \n",
    "        - Conditional Random Fields (CRFs)\n",
    "\n",
    "<hr>\n",
    "\n",
    "The ATIS (Airline Travel Information Systems) dataset consists of English language queries for booking (or requesting information about) flights in the US. \n",
    "\n",
    "Each word in a query (i.e. a request by a user) is labelled according to its **entity-type**, for e.g. in the query 'please show morning flights from chicago to new york', 'chicago' and 'new york are labelled as 'source' and 'destination' locations respectively while 'morning' is labelled as 'time-of-day' (the exact labelling scheme is a bit different, more on that later).\n",
    "\n",
    "Some example queries taken from the dataset are shown below:\n",
    "\n",
    "```\n",
    "{\n",
    "'what flights leave atlanta at about DIGIT in the afternoon and arrive in san francisco',\n",
    " 'what is the abbreviation for canadian airlines international',\n",
    " \"i 'd like to know the earliest flight from boston to atlanta\",\n",
    " 'show me the us air flights from atlanta to boston',\n",
    " 'show me the cheapest round trips from dallas to baltimore',\n",
    " \"i 'd like to see all flights from denver to philadelphia\"\n",
    " }\n",
    " ```\n",
    "\n",
    "### Objective\n",
    "Our objective is to **build an information extraction system** which can extract entities relevant for booking flights (such as source and destination cities, time, date, budget constraints etc.) in a **structured format** from a given user-generated query.\n",
    "\n",
    "A structured format could be a dictionary, a JSON, etc. - basically anything that can be parsed and used for looking up relevant flights from a database.\n",
    "\n",
    "\n",
    "### Downloads\n",
    "The dataset is divided into five folds, each fold having a training, validation and test set.\n",
    "You can download the dataset here: http://lisaweb.iro.umontreal.ca/transfert/lisa/users/mesnilgr/atis/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Data \n",
    "\n",
    "Let's understand the structure of the training data. The dataset is provided in five folds, each fold having a training, validation, test set and a dict (explained later). All folds are structurally identical, so understanding one fold is enough to understand the entire set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libaries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk, pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import gzip, os, pickle # gzip for reading the gz files, pickle to save/dump trained model \n",
    "import _pickle as cPickle\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn_crfsuite import scorers\n",
    "\n",
    "from nltk.corpus import conll2000\n",
    "from nltk import conlltags2tree, tree2conlltags, ChunkParserI\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "# supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now read the first fold of the dataset. The data is in .gz files, so we'll need the gzip library as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read the first part of the dataset\n",
    "# each part (.gz file) contains train, validation and test sets, plus a dict\n",
    "\n",
    "filename = './data/atis.fold0.pkl.gz'\n",
    "f = gzip.open(filename, 'rb')\n",
    "try:\n",
    "    train_set, valid_set, test_set, dicts = pickle.load(f, encoding='latin1')\n",
    "except:\n",
    "    train_set, valid_set, test_set, dicts = pickle.load(f)\n",
    "finally:\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "\n",
      "<class 'list'> <class 'list'> <class 'list'>\n",
      "3983 3983 3983\n"
     ]
    }
   ],
   "source": [
    "# type and size of the train set\n",
    "print(type(train_set))\n",
    "print()\n",
    "\n",
    "# types of the three elements in the tuple\n",
    "print(type(train_set[0]), type(train_set[1]), type(train_set[2]))\n",
    "print(len(train_set[0]), len(train_set[1]), len(train_set[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set is a tuple containing three lists of same lengths as shown above. Similarly, the validation and test sets contain three lists as well (shown below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'list'> <class 'list'>\n",
      "995 995 995\n",
      "\n",
      "<class 'list'> <class 'list'> <class 'list'>\n",
      "893 893 893\n"
     ]
    }
   ],
   "source": [
    "# validation set\n",
    "print(type(valid_set[0]), type(valid_set[1]), type(valid_set[2]))\n",
    "print(len(valid_set[0]), len(valid_set[1]), len(valid_set[2]))\n",
    "print()\n",
    "\n",
    "# test set\n",
    "print(type(test_set[0]), type(test_set[1]), type(test_set[2]))\n",
    "print(len(test_set[0]), len(test_set[1]), len(test_set[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we have the train, validation and test sets each containing three lists of different lengths. Now, let's understand the data stored in the three lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([554, 194, 268,  64,  62,  16,   8, 234, 481,  20,  40,  58, 234,\n",
      "       415, 205], dtype=int32),\n",
      " array([554, 241, 481,  14, 200,  91,  26, 239], dtype=int32),\n",
      " array([232,   0, 273, 502, 254, 481, 165, 193, 208,  77, 502,  64],\n",
      "      dtype=int32)]\n",
      "##################################################\n",
      "[array([  0,   0,   0,  18,   0,   1,  52,   0,   0,  76,   0,   0,   0,\n",
      "        18, 109], dtype=int32),\n",
      " array([  0,   0,   0,   0,   0,   6, 107, 107], dtype=int32),\n",
      " array([ 0,  0,  0,  0,  0,  0, 44,  0,  0, 18,  0, 18], dtype=int32)]\n",
      "##################################################\n",
      "[array([126, 126, 126,  48, 126,  36,  35, 126, 126,  33, 126, 126, 126,\n",
      "        78, 123], dtype=int32),\n",
      " array([126, 126, 126, 126, 126,   2,  83,  83], dtype=int32),\n",
      " array([126, 126, 126, 126, 126, 126,  42, 126, 126,  48, 126,  78],\n",
      "      dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "# first few elements in each list of the training set \n",
    "pprint.pprint(train_set[0][:3])\n",
    "print('#'*50)\n",
    "pprint.pprint(train_set[1][:3])\n",
    "print('#'*50)\n",
    "pprint.pprint(train_set[2][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first list contains the actual queries encoded by integers such as 554, 194, 268 ... and so on. For e.g. the first three integers 554, 194, 268 are encoded values of the words 'what', 'flights', 'leave' etc. \n",
    "\n",
    "\n",
    "The second list is to be ignored.\n",
    "\n",
    "The third list contains the (encoded) label of each word (how to do these mappings is explained in detail below). Since the actual words are encoded by numbers, we have to decode them using the dicts provided. Let's first store the three lists into separate objects so we don't have to worry about indexing the lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the three elements of the tuple in three objects \n",
    "# The '_' is a conventional variable in python used to store non-useful/dummy objects\n",
    "train_x, _, train_label = train_set\n",
    "val_x, _, val_label = valid_set\n",
    "test_x, _, test_label = test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, for training, validation and test sets, we have the **encoded words and labels** stored in the lists (train_x, train_label), (val_x, val_label) and (test_x, test_label). The first list represents the actual words (encoded), and the other list contains their labels (again, encoded).\n",
    "\n",
    "Let's now understand the structure of the lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([554, 194, 268,  64,  62,  16,   8, 234, 481,  20,  40,  58, 234,\n",
       "       415, 205], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each list in the tuple is a numpy array (which us a complete sentence/query)\n",
    "# printing first list in the tuple's first element\n",
    "# each element represents a word of the query\n",
    "# this translates to 'what flights leave atlanta ....'\n",
    "train_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([126, 126, 126,  48, 126,  36,  35, 126, 126,  33, 126, 126, 126,\n",
       "        78, 123], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels are stored in the third list train_label\n",
    "train_label[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To map the integers to words, we need to use the dictionaries provided. The dicts ```words2idx``` and ```labels2idx``` map the numeric ids to the actual words and labels respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['labels2idx', 'tables2idx', 'words2idx'])\n"
     ]
    }
   ],
   "source": [
    "# dicts to map numbers to words/labels\n",
    "print(type(dicts))\n",
    "print(dicts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing labels and words in separate variables\n",
    "# we'll need only two of these dicts - words and labels\n",
    "words = dicts['words2idx']\n",
    "labels = dicts['labels2idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('much', 324),\n",
       " ('las', 260),\n",
       " ('many', 296),\n",
       " ('requesting', 402),\n",
       " ('also', 34),\n",
       " ('morning', 321),\n",
       " ('thursdays', 497),\n",
       " ('their', 482),\n",
       " ('c', 87),\n",
       " ('ord', 364)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each key of 'words' is a word, each value its index\n",
    "# printing some random key:value pairs of 'words'\n",
    "random.sample(words.items(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'flights',\n",
       " 'leave',\n",
       " 'atlanta',\n",
       " 'at',\n",
       " 'about',\n",
       " 'DIGIT',\n",
       " 'in',\n",
       " 'the',\n",
       " 'afternoon',\n",
       " 'and',\n",
       " 'arrive',\n",
       " 'in',\n",
       " 'san',\n",
       " 'francisco']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, we can map the numeric values v in a sentence with the k,v in the dict\n",
    "# train_x contains the list of training queries; train_x[0] is the first query\n",
    "# this is the first query\n",
    "[k for val in train_x[0] for k,v in words.items() if v==val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what flights leave atlanta at about DIGIT in the afternoon and arrive in san francisco',\n",
       " 'what is the abbreviation for canadian airlines international',\n",
       " \"i 'd like to know the earliest flight from boston to atlanta\",\n",
       " 'show me the us air flights from atlanta to boston',\n",
       " 'show me the cheapest round trips from dallas to baltimore',\n",
       " \"i 'd like to see all flights from denver to philadelphia\",\n",
       " 'explain fare code qx',\n",
       " \"i 'd like a united airlines flight on wednesday from san francisco to boston\",\n",
       " 'what is the price of american airlines flight DIGITDIGIT from new york to los angeles',\n",
       " 'what does the meal code s stand for',\n",
       " 'what are all flights to denver from philadelphia on sunday',\n",
       " 'what times does the late afternoon flight leave from washington for denver',\n",
       " 'what flights are available monday from san francisco to pittsburgh',\n",
       " 'what airlines have business class',\n",
       " 'flights from atlanta to washington dc',\n",
       " 'from new york to toronto on thursday morning',\n",
       " 'show me all the direct flights from atlanta to baltimore',\n",
       " 'list the flights from new york to miami on a tuesday which are nonstop and cost less than DIGITDIGITDIGIT dollars',\n",
       " 'show me the first flight that arrives in toronto from cincinnati',\n",
       " 'what planes are used by twa',\n",
       " 'please give me the prices for all flights from philadelphia to denver airport next sunday',\n",
       " 'show me all flights from pittsburgh to oakland that arrive after DIGITDIGIT am',\n",
       " 'what is the least expensive flight today from atlanta to san francisco',\n",
       " 'i want a flight from philadelphia to dallas with a stop in atlanta',\n",
       " 'show me the flights from baltimore to philadelphia',\n",
       " 'what airlines fly from st. petersburg to milwaukee and from milwaukee to tacoma',\n",
       " 'please give me the flights from san francisco to washington dc',\n",
       " 'i need a flight delta airlines kansas city to salt lake',\n",
       " 'show me flights going from boston to denver arriving on wednesday morning',\n",
       " 'show me flights leaving from denver colorado to pittsburgh pennsylvania on wednesdays after DIGIT pm']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at the first few queries\n",
    "sents = []\n",
    "for i in range(30):\n",
    "    sents.append(' '.join([k for val in train_x[i] for k,v in words.items() if v==val]))\n",
    "\n",
    "sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can map the encoded values of each word's label using the ```labels``` dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B-mod', 54),\n",
       " ('B-depart_date.date_relative', 25),\n",
       " ('B-arrive_time.time', 14),\n",
       " ('I-restriction_code', 113),\n",
       " ('I-transport_type', 125),\n",
       " ('B-return_date.day_name', 60),\n",
       " ('B-month_name', 55),\n",
       " ('B-arrive_time.period_of_day', 12),\n",
       " ('B-depart_time.end_time', 31),\n",
       " ('B-meal_description', 53),\n",
       " ('B-restriction_code', 58),\n",
       " ('I-flight_mod', 104),\n",
       " ('I-flight_number', 105),\n",
       " ('B-return_date.month_name', 62),\n",
       " ('B-stoploc.airport_code', 69),\n",
       " ('B-compartment', 19),\n",
       " ('I-depart_time.end_time', 96),\n",
       " ('B-depart_time.period_of_day', 33),\n",
       " ('B-depart_time.period_mod', 32),\n",
       " ('I-depart_date.day_number', 94),\n",
       " ('I-return_date.date_relative', 114),\n",
       " ('I-fromloc.city_name', 109),\n",
       " ('B-arrive_date.day_name', 6),\n",
       " ('I-depart_time.start_time', 98),\n",
       " ('B-arrive_time.end_time', 10)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels dict contains IOB (inside-out-beginning) labelled entities\n",
    "# printing some randomg k:v pairs \n",
    "random.sample(labels.items(), 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 127 classes of labels (including the 'O' - tokens that do not fall into any entity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n"
     ]
    }
   ],
   "source": [
    "# number of labels\n",
    "print(len(labels.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reversing the Dictionaries\n",
    "\n",
    "Since the dicts ```words``` and ```labels``` are key:value pairs of index:word/label, let's reverse the dicts so that we don't have to do a reverse lookup everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting words_to_id to id_to_words\n",
    "# and labels_to_id to id_to_labels\n",
    "id_to_words = {words[k]:k for k in words}\n",
    "id_to_labels = {labels[k]:k for k in labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print the words and corresponding labels simply by looking up the value of a numeric index of each word. Let's write a function which takes in an index and returns the corresponding query with its labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in an integer index corresponding to a query \n",
    "# and returns a list of (word, label) pairs   \n",
    "def print_query(index):\n",
    "    w = [id_to_words[id] for id in train_x[index]]\n",
    "    l = [id_to_labels[id] for id in train_label[index]]\n",
    "    return list(zip(w, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what', 'O'),\n",
       " ('flights', 'O'),\n",
       " ('leave', 'O'),\n",
       " ('atlanta', 'B-fromloc.city_name'),\n",
       " ('at', 'O'),\n",
       " ('about', 'B-depart_time.time_relative'),\n",
       " ('DIGIT', 'B-depart_time.time'),\n",
       " ('in', 'O'),\n",
       " ('the', 'O'),\n",
       " ('afternoon', 'B-depart_time.period_of_day'),\n",
       " ('and', 'O'),\n",
       " ('arrive', 'O'),\n",
       " ('in', 'O'),\n",
       " ('san', 'B-toloc.city_name'),\n",
       " ('francisco', 'I-toloc.city_name')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample query\n",
    "print_query(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, some queries specify stopover cities, such as this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 'O'),\n",
       " ('there', 'O'),\n",
       " ('a', 'O'),\n",
       " ('flight', 'O'),\n",
       " ('between', 'O'),\n",
       " ('oakland', 'B-fromloc.city_name'),\n",
       " ('and', 'O'),\n",
       " ('boston', 'B-toloc.city_name'),\n",
       " ('with', 'O'),\n",
       " ('a', 'O'),\n",
       " ('stopover', 'O'),\n",
       " ('in', 'O'),\n",
       " ('dallas', 'B-stoploc.city_name'),\n",
       " ('fort', 'I-stoploc.city_name'),\n",
       " ('worth', 'I-stoploc.city_name'),\n",
       " ('on', 'O'),\n",
       " ('twa', 'B-airline_code')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example query: stopover city\n",
    "print_query(3443)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Extraction \n",
    "\n",
    "**Information Extraction (IE)** refers to the task of extracting structured information from unstructured text data. In this case, we want to extract all pieces of information from a query which are useful in making a flight reservation, such as source and destination cities, date of travel, price range etc. \n",
    "\n",
    "Other examples of IE tasks are extracting information about stock market announcements from financial news (which could be useful for predicting stock prices etc.), extracting structured information from large corpora of documents such as encyclopedias, government documents etc. On wikipedia, for e.g., some structured information is shown on the right side of the pages:\n",
    "\n",
    "<br><br><hr>\n",
    "<img src='https://i.stack.imgur.com/oJumb.png'>\n",
    "<br><br><hr>\n",
    "\n",
    "Most IE tasks start with the task of **Named Entity Recognition (NER)** - identifying mentions of *entities* in the text. Loosely speaking, entities refer to names of people, organizations (e.g. Air India, United Airlines), places/cities (Mumbai, Chicago), dates and timepoints (May, Wednesday, morning flight), numbers of specific types (e.g. money - 5000 INR) etc.\n",
    "\n",
    "The general process of information extraction is described below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Information Extraction Pipeline\n",
    "\n",
    "Most IE pipelines start with the usual text preprocessing steps - sentence segmentation, word tokenisation and POS tagging. After preprocessing, the usual tasks are named entity recognition, and optionally relation recognition. \n",
    "\n",
    "Note that this is a generic pipeline, and you may make modifications according to the nature of your application. For example, you may add a 'spell check/correction layer' as the first preprocessing step if you expect some input data to have spelling errors.\n",
    "\n",
    "A generic IE pipeline schema, taken from the official NLTK book, is shown below.\n",
    "\n",
    "<br><br><hr>\n",
    "<img src='https://www.nltk.org/images/ie-architecture.png'>\n",
    "<br><br><hr>\n",
    "\n",
    "### Preprocessing \n",
    "\n",
    "The usual preprocessing steps are - if the raw input data is in the form of paragraphs, it is converted into sentences using a **sentence segmenter**, then broken down into tokens using **tokenisation**, and finally each token is **POS tagged**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)\n",
    "\n",
    "The labels corresponding to each word/token, as shown above, are of three types - I, O and B, which stand for inside, out and beginning (called **IOB tags**). This is a common way of labelling text data meant for NER tasks. The task of information extraction and named-entity recognition is explained in detail below. First, let's understand the task of NER and IOB labelling in detail.\n",
    "\n",
    "Some example IOB tagged words are shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what', 'O'),\n",
       " ('is', 'O'),\n",
       " ('the', 'O'),\n",
       " ('fare', 'O'),\n",
       " ('on', 'O'),\n",
       " ('november', 'B-depart_date.month_name'),\n",
       " ('seventh', 'B-depart_date.day_number'),\n",
       " ('going', 'O'),\n",
       " ('one', 'B-round_trip'),\n",
       " ('way', 'I-round_trip'),\n",
       " ('from', 'O'),\n",
       " ('pittsburgh', 'B-fromloc.city_name'),\n",
       " ('to', 'O'),\n",
       " ('philadelphia', 'B-toloc.city_name')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run multiple times to see samples\n",
    "# randomly chosen sample IOB tagged queries from training data\n",
    "i=random.randrange(len(train_x))\n",
    "print_query(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the query above, there are two named entities (cities) - san francisco and denver. \n",
    "\n",
    "Since san francisco comprises of two words, the first one is tagged *B-* and the second as *I-*. On the other hand, denver is only one word, so there's no *I-* tag. All the other words are not entities and are thus marked *O* (outside any entity).\n",
    "\n",
    "The **NER task** is to **predict the IOB labels** of each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER as a Sequence-Labelling Task\n",
    "\n",
    "The task of **training an NER system**, i.e. assigning an IOB label to each word, is a **sequence labelling task** similar to POS tagging. For sequence labelling, one can try rule-based models such as writing **regular-expression based rules** to extract entities, **chunking** patterns of POS tags into an 'entity chunk' etc. (we'll try some of these below)\n",
    "\n",
    "One the other hand, one can use **probabilistic sequence labelling models** such as **HMMs**, the **Naive Bayes** classifier (classifying each word into one label class), **Conditional Random Fields (CRFs)** etc. \n",
    "\n",
    "Once the IOB tags of each word are predicted, we can **evaluate the model** using the usual metrics for multi-class classification models (num_classes = number of IOB tags).\n",
    "\n",
    "In the upcoming sections, we will try some of these approaches and compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models for Entity Recognition\n",
    "\n",
    "In the following sections, we'll build a variety of models for entity recognition, i.e. to predict the sequence of IOB tag of words. We'll try the two broad approaches - **rule-based models** and **probabilistic models**. \n",
    "\n",
    "Before that, we need to do some basic preprocessing of the data. \n",
    "\n",
    "## Part of Speech Tagging\n",
    "\n",
    "The usual preprocessing steps are sentence segmentation, tokenisation and POS tagging, but since in this case the raw data is already split into sentences (queries) and words, we only need to do POS tagging.\n",
    "\n",
    "The function below takes in a list of (encoded) sentences, uses the dict ```id_to_words``` to decode the numeric to the corresponding word, and returns the POS tagged list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mz195/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# POS tagging sentences\n",
    "# takes in a list of sentences and returns a list of POS-tagged sentences\n",
    "# in the form (word, tag)\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "def pos_tag(sent_list):\n",
    "    pos_tags = []    \n",
    "    for sent in sent_list:\n",
    "        tagged_words = nltk.pos_tag([id_to_words[val] for val in sent])\n",
    "        pos_tags.append(tagged_words)\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos tagging train, validation and test sets\n",
    "train_pos = pos_tag(train_x)\n",
    "valid_pos = pos_tag(val_x)\n",
    "test_pos = pos_tag(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('please', 'VB'),\n",
       " ('give', 'VB'),\n",
       " ('me', 'PRP'),\n",
       " ('ground', 'JJ'),\n",
       " ('transportation', 'NN'),\n",
       " ('information', 'NN'),\n",
       " ('between', 'IN'),\n",
       " ('dallas', 'NN'),\n",
       " ('fort', 'NN'),\n",
       " ('worth', 'JJ'),\n",
       " ('airport', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('downtown', 'NN'),\n",
       " ('dallas', 'NNS')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at tags of some randomly chosen queries\n",
    "# notice that most cities after 'TO' are incorrectly tagged as VB\n",
    "i = random.randrange(len(train_pos))\n",
    "train_pos[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problems with the NLTK Tagger**\n",
    "\n",
    "Note that almost all city/airport names that come after 'to/TO' are tagges as verbs 'VB', which is clearly incorrect. This is because NLTK's built-in tagger is trained using the penntreebank dataset, and it takes 'to/TO' as a strong signal for a 'VB'.\n",
    "\n",
    "\n",
    "In general, the performance of a POS tagger depends a lot on the data used to train it. There are alternatives to it - one, you can try using an alternative tagger such as the Stanford tagger, Spacy etc. (though note that getting them up and running in python may take a bit of time in installing dependencies/debugging etc.).\n",
    "\n",
    "The other alternative (recommended as a quick fix) is to use a **backup tagger** within NLTK, i.e. manually specify a unigram/bigram tagger to be used, and backed up by the standard NLTK tagger. You can <a href=\"https://stackoverflow.com/questions/5919355/custom-tagging-with-nltk/5922373#5922373\">learn how to do that here.</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating 3-tuples of ```(word, pos, IOS_label)```\n",
    "To train a model, we need the entity labels of each word along with the POS tags, for e.g. in this format:\n",
    "\n",
    "```\n",
    "('show', 'VB', 'O'),\n",
    "('me', 'PRP', 'O'),\n",
    "('the', 'DT', 'O'),\n",
    "('cheapest', 'JJS', 'B-cost_relative'),\n",
    "('round', 'NN', 'B-round_trip'),\n",
    "('trips', 'NNS', 'I-round_trip'),\n",
    "('from', 'IN', 'O'),\n",
    "('dallas', 'NN', 'B-fromloc.city_name'),\n",
    "('to', 'TO', 'O'),\n",
    "('baltimore', 'VB', 'B-toloc.city_name')\n",
    "```\n",
    "<hr>\n",
    "\n",
    "Let's convert the training, validation and test sentences to this form. Since we have already  done POS tagging of the queries, we'll write a function which takes queries in the form (word, pos_tag) and the labels as input, and returns the list of sentences in the form (word, pos_tag, iob_label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create (word, pos_tag, iob_label) tuples for a given dataset\n",
    "def create_word_pos_label(pos_tagged_data, labels):\n",
    "    iob_labels = []         # initialize the list of 3-tuples to be returned\n",
    "    \n",
    "    for sent in list(zip(pos_tagged_data, labels)):\n",
    "        pos = sent[0]       \n",
    "        labels = sent[1]    \n",
    "        zipped_list = list(zip(pos, labels)) # [(word, pos), label]\n",
    "        \n",
    "        # create (word, pos, label) tuples from zipped list\n",
    "        tuple_3 = [(word_pos_tuple[0], word_pos_tuple[1], id_to_labels[label]) \n",
    "                   for word_pos_tuple, label in zipped_list]\n",
    "        iob_labels.append(tuple_3)\n",
    "    return iob_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('show', 'VB', 'O'),\n",
       "  ('me', 'PRP', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('cheapest', 'JJS', 'B-cost_relative'),\n",
       "  ('round', 'NN', 'B-round_trip'),\n",
       "  ('trips', 'NNS', 'I-round_trip'),\n",
       "  ('from', 'IN', 'O'),\n",
       "  ('dallas', 'NN', 'B-fromloc.city_name'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('baltimore', 'VB', 'B-toloc.city_name')],\n",
       " [('i', 'JJ', 'O'),\n",
       "  (\"'d\", 'MD', 'O'),\n",
       "  ('like', 'VB', 'O'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('see', 'VB', 'O'),\n",
       "  ('all', 'DT', 'O'),\n",
       "  ('flights', 'NNS', 'O'),\n",
       "  ('from', 'IN', 'O'),\n",
       "  ('denver', 'NN', 'B-fromloc.city_name'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('philadelphia', 'VB', 'B-toloc.city_name')]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing some sample queries in the form (word, pos, label)\n",
    "train_labels = create_word_pos_label(train_pos, train_label)\n",
    "train_labels[4:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing validation and test data as well as (word, pos, label)\n",
    "valid_labels = create_word_pos_label(valid_pos, val_label)\n",
    "test_labels = create_word_pos_label(test_pos, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have converted the queries in the form (word, pos, label), we can convert them into a **tree format** and observe the actual entities more clearly (rather than IOB labels). \n",
    "\n",
    "In general, IOB tagged sentences are represented in either of the two common formats - 1. The ```(word, pos, label)``` or the tagged list format or 2. The tree format. \n",
    "\n",
    "As we will see later, some built-in models/sequence taggers in NLTK need the data in the tree format.\n",
    "\n",
    "### Converting List to Tree Format\n",
    "\n",
    "Let's now convert the sentences into a tree format, which is needed by NLTK to train taggers. In NLTK, there are two main methods to convert between the two formats (the list of tags and tree) - ```conlltags2tree``` and ```tree2conlltags```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('show', 'VB', 'O'),\n",
       " ('me', 'PRP', 'O'),\n",
       " ('the', 'DT', 'O'),\n",
       " ('us', 'PRP', 'B-airline_name'),\n",
       " ('air', 'NN', 'I-airline_name'),\n",
       " ('flights', 'NNS', 'O'),\n",
       " ('from', 'IN', 'O'),\n",
       " ('atlanta', 'NN', 'B-fromloc.city_name'),\n",
       " ('to', 'TO', 'O'),\n",
       " ('boston', 'VB', 'B-toloc.city_name')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a sample tree in tuple format\n",
    "train_labels[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  show/VB\n",
      "  me/PRP\n",
      "  the/DT\n",
      "  (airline_name us/PRP air/NN)\n",
      "  flights/NNS\n",
      "  from/IN\n",
      "  (fromloc.city_name atlanta/NN)\n",
      "  to/TO\n",
      "  (toloc.city_name boston/VB))\n"
     ]
    }
   ],
   "source": [
    "# converting the sample sentence above to tree format\n",
    "tree = conlltags2tree(train_labels[3])\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree above shows three entities in the query - ```flight_mod earliest``` (earliest), ```fromloc.city_name``` (boston), ```toloc.city_name``` (atlanta)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now convert all training sentences to trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting training, validation and test datasets to tree format\n",
    "train_trees = [conlltags2tree(sent) for sent in train_labels]\n",
    "valid_trees = [conlltags2tree(sent) for sent in valid_labels]\n",
    "test_trees = [conlltags2tree(sent) for sent in test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  what/WP\n",
      "  (flight_time times/VBZ)\n",
      "  does/VBZ\n",
      "  the/DT\n",
      "  (depart_time.period_of_day late/JJ afternoon/NN)\n",
      "  flight/NN\n",
      "  leave/VBP\n",
      "  from/IN\n",
      "  (fromloc.city_name washington/NN)\n",
      "  for/IN\n",
      "  (toloc.city_name denver/NN))\n"
     ]
    }
   ],
   "source": [
    "# print some sample training trees\n",
    "i=random.randrange(len(train_trees))\n",
    "print(train_trees[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the queries in tree formats, we can build some models to extract entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-Based Models for Entity Recognition\n",
    "\n",
    "The most basic rule-based system can be written using regular expressions. The idea is to manually identify patterns which indicate occurence of entities we are interested in, such as source and destination cities, mentions of dates and time, names of organisations (in this case airlines such as united airlines, american airlines etc.) and write regular expressions to match them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "Chunking is a way to identify meaningful sequences of tokens called chunks in a sentence. It is commonly used to identify sequences of nouns, verbs etc. For example, in the example sentence taken from the NLTK book: <br>\n",
    "\n",
    "S = *\"We saw the yellow dog\"*\n",
    "\n",
    "there are two **noun phrase chunks** as shown below. Each outer box represents a chunk.\n",
    "\n",
    "<img src='https://www.nltk.org/book/tree_images/ch07-tree-1.png'>\n",
    "\n",
    "The corresponding **IOB representation** of the same is as follows:\n",
    "\n",
    "<img src='https://www.nltk.org/images/chunk-tagrep.png'>\n",
    "\n",
    "\n",
    "\n",
    "Similarly, in our dataset, the following sentence contains chunks such as fromloc.city_name (san francisco), class_type (first class), depart_time.time (DIGITDIGIT noon) etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  show/VB\n",
      "  me/PRP\n",
      "  flights/NNS\n",
      "  on/IN\n",
      "  (depart_date.day_name sunday/NN)\n",
      "  going/VBG\n",
      "  from/IN\n",
      "  (fromloc.city_name san/JJ francisco/NN)\n",
      "  to/TO\n",
      "  (toloc.city_name boston/VB)\n",
      "  (flight_stop nonstop/JJ)\n",
      "  (class_type first/JJ class/NN)\n",
      "  leaving/NN\n",
      "  (depart_time.time_relative after/IN)\n",
      "  (depart_time.time DIGITDIGIT/NNP noon/NN))\n"
     ]
    }
   ],
   "source": [
    "# sample chunks \n",
    "print(train_trees[3468])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various techniques for building chunkers, such as regex based, unigram and bigram chunkers etc.\n",
    "\n",
    "### Regular Expression Based Chunkers\n",
    "\n",
    "Regex based chunkers define what is called a **chunk grammar**. A **chunk grammar is a pattern of POS tags** which are likely to form a particular chunk (and thus POS tagging is a necessary preprocessing step for such chunkers). \n",
    "\n",
    "The example from the NLTK book defined a simple garmmar to identify noun phrase chunks:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking example sentence\n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
    "            (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), (\"the\", \"DT\"), (\"cat\", \"NN\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we **define a chunk grammar** to identify noun phrase chunks as follows: *A noun phrase chunk occurs when an optional determiner (DT) is followed by any number of adjectives (JJ) and then a noun (NN).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define chunk grammar to identify noun phrase chunks\n",
    "# an optional determiner (DT), followed by any number of adjectives (JJ) \n",
    "# and then a noun (NN)\n",
    "grammar = \"NP_chunk: {<DT>?<JJ>*<NN>}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the ```nltk.RegexpParser``` to parse the given sentence. The output will be a tree which identifies the NP chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP_chunk the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP_chunk the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "# parse the sentence\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(sentence)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first NP chunk is 'the little yellow dog' and the second one is 'the cat'. One can also print the list representation of this tree as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'DT', 'B-NP_chunk'),\n",
       " ('little', 'JJ', 'I-NP_chunk'),\n",
       " ('yellow', 'JJ', 'I-NP_chunk'),\n",
       " ('dog', 'NN', 'I-NP_chunk'),\n",
       " ('barked', 'VBD', 'O'),\n",
       " ('at', 'IN', 'O'),\n",
       " ('the', 'DT', 'B-NP_chunk'),\n",
       " ('cat', 'NN', 'I-NP_chunk')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list form of the tree\n",
    "tree2conlltags(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can write rules to identify entities in the airlines dataset. Before building any useful chunkers, let's first make a **baseline chunker** - one which assigns the label 'O' to every word and see its evaluation metrics. We can then compare the eventual models with this one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  63.8%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n"
     ]
    }
   ],
   "source": [
    "# baseline (dummy chunker)\n",
    "# assigns 'O' to each word\n",
    "\n",
    "grammar = ''\n",
    "\n",
    "# initialise cp \n",
    "cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "# evaluate results against actual IOB labels\n",
    "result = cp.evaluate(train_trees)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of 63.8% shows that about 63% words have the tag 'O', i.e. they do not fall in any chunk/entity. The other metrics are 0 since we did not predict any chunks at all.\n",
    "\n",
    "Let's now look at a few sentences and try to identify patterns for the chunks ```fromloc.city_name``` and ```toloc.city_name``` (since these are the most common entities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  all/DT\n",
      "  flights/NNS\n",
      "  from/IN\n",
      "  (fromloc.city_name montreal/NN)\n",
      "  (cost_relative less/JJR)\n",
      "  than/IN\n",
      "  (fare_amount DIGITDIGITDIGIT/CD dollars/NNS))\n"
     ]
    }
   ],
   "source": [
    "# sample queries \n",
    "print(train_trees[random.randrange(len(train_trees))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to see that a common pattern is *from city_1 to city_2*. The POS tag of from is 'IN', to is 'TO'. However, there's an interesting flaw in the tagger - most cities (city_2) after TO are tagged as as verb 'VB' (the nltk tagger uses 'TO' as a string signal for a verb).\n",
    "\n",
    "Nevertheless, the pattern is ```from/IN city_1/JJ city_1/NN to/TO city_2/VB city_2/NN``` (you can try looking at multiple sentences and verify).\n",
    "\n",
    "Let's define the grammar to identify the two types of chunks - ```fromloc.city_name``` and ```toloc.city_name```. The syntax is to simply write two regexes within the grammar, one for each, one after the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  65.2%%\n",
      "    Precision:     31.7%%\n",
      "    Recall:        40.9%%\n",
      "    F-Measure:     35.7%%\n"
     ]
    }
   ],
   "source": [
    "# grammar for source and destination city chunks\n",
    "grammar = '''\n",
    "fromloc.city_name: {<JJ>?<NN>}\n",
    "toloc.city_name: {<VB><NN>?}\n",
    "'''\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.evaluate(train_trees)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are although better than the baseline model, they are still quite unimpressive. Notice that now precision, recall and f-score are non-zero, indicating that the chunker is able to identify at least some chunks correctly (in this case ```fromloc.city_name``` and ```toloc.city_name```).\n",
    "\n",
    "We can add more regex patterns for other chunk types as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in this dataset, queries are quite complex (large variety of labels, sentence structures etc.) and thus it is extremely hard write hand-written rules to extract useful entities.\n",
    "\n",
    "Thus, we need to train probabilistic models such as CRFs, HMMs etc. to tag each word with its corresponding entity label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Models for Entity Recognition\n",
    "\n",
    "Let's experiment with a few different models for labelling words with named entities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Chunker\n",
    "\n",
    "Let's now try a **unigram chunker**. A unigram chunker assigns the IOB label that is most likely for each POS tag.\n",
    "\n",
    "The following code defines a class ```UnigramChunker``` which (on initialisation) first converts the tree form of a sentence to the list form (word, pos, label), extracts the (pos, label) pairs and computes the unigram probabilities ```P(label | pos)``` for each POS tag. It then simply assigns the label that is most likely for the POS tag.\n",
    "\n",
    "The ```parse()``` method of the class takes a sentence in the form (word, pos) as the input, extracts only the pos tag from it, and uses the unigram tagger to assign the IOB label to each word. It then returns the sentence after converting it to a tree format.\n",
    "\n",
    "Note that the unigram tagger, like the previous regex-based chunkers, *does not make use of the word itself but only the word's POS tag*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram chunker\n",
    "\n",
    "class UnigramChunker(ChunkParserI):    \n",
    "    def __init__(self, train_sents):\n",
    "        # convert train sents from tree format to tags\n",
    "        train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)] \n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        \n",
    "        # convert to tree again\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag) in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  66.3%%\n",
      "    Precision:     37.5%%\n",
      "    Recall:        18.5%%\n",
      "    F-Measure:     24.8%%\n"
     ]
    }
   ],
   "source": [
    "# unigram chunker \n",
    "unigram_chunker = UnigramChunker(train_trees)\n",
    "print(unigram_chunker.evaluate(valid_trees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy, precision and recall have slightly improved compared to the previous regex-based  parser. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at what the unigram parser has learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CC', 'O'), ('CD', 'B-round_trip'), ('DT', 'O'), ('EX', 'O'), ('FW', 'B-fromloc.city_name'), ('IN', 'O'), ('JJ', 'O'), ('JJR', 'B-cost_relative'), ('JJS', 'B-cost_relative'), ('MD', 'O'), ('NN', 'O'), ('NNP', 'B-depart_time.time'), ('NNS', 'O'), ('PDT', 'O'), ('POS', 'O'), ('PRP', 'O'), ('PRP$', 'O'), ('RB', 'O'), ('RBR', 'B-cost_relative'), ('RBS', 'B-cost_relative'), ('RP', 'O'), ('TO', 'O'), ('VB', 'B-toloc.city_name'), ('VBD', 'O'), ('VBG', 'O'), ('VBN', 'O'), ('VBP', 'O'), ('VBZ', 'O'), ('WDT', 'O'), ('WP', 'O'), ('WRB', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# printing the most likely IOB tags for each POS tag\n",
    "\n",
    "# extract the list of pos tags\n",
    "postags = sorted(set([pos for sent in train_trees for (word, pos) in sent.leaves()]))\n",
    "\n",
    "# for each tag, assign the most likely IOB label\n",
    "print(unigram_chunker.tagger.tag(postags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unigram tagger has learnt that most pos tags are indeed an 'O', i.e. don't form an entity. Some interesting patterns it has learnt are:\n",
    "- JJR, JJS (relative adjectives), are most likely B-cost_relative (e.g. cheapest, cheaper)\n",
    "- NNP is most likely to be B-depart_time.time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Chunker\n",
    "\n",
    "Let's try a bigram chunker as well - we just need to change the ```UnigramTagger``` to ```BigramTagger```. This works exactly like the unigram chunker, the only difference being that now the probability of a pos tag having a label is computed using the current and the previous POS tags, i.e. P(label | pos, prev_pos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram tagger\n",
    "\n",
    "class BigramChunker(ChunkParserI):    \n",
    "    def __init__(self, train_sents):\n",
    "        # convert train sents from tree format to tags\n",
    "        train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)] \n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data)\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        \n",
    "        # convert to tree again\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag) in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  70.6%%\n",
      "    Precision:     43.5%%\n",
      "    Recall:        38.8%%\n",
      "    F-Measure:     41.0%%\n"
     ]
    }
   ],
   "source": [
    "# unigram chunker \n",
    "bigram_chunker = BigramChunker(train_trees)\n",
    "print(bigram_chunker.evaluate(valid_trees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics have improved significantly from unigram to bigram, which is expected. However, there are still some major flaws in this approach to build chunkers, the main drawback being that the model *uses only the POS tag to assign the label, not the actual word itself*. \n",
    "\n",
    "It is likely that if a model can make use of the word itself apart from the POS tag, it should be able to learn more complex patterns needed for this task. \n",
    "\n",
    "In fact, apart from the word, we can extract a large number of other features, such as previous word, previous tag, whether the word is a numeric, whether the word is a city or an airline company etc.\n",
    "\n",
    "Thus, in the following few sections, we'll extract a variety of features and build classifiers such as Naive Bayes using those features. \n",
    "\n",
    "Our first step in the direction of feature extraction will be to extract an feature which indicates whether a word is a city, state or county etc. Such features can be extracted by simply **looking up a gazetteer**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Gazetteer to Lookup Cities and States\n",
    "\n",
    "A gazetteer is a geographical directory which stores data regarding the names of geographical entities (cities, states, countries) and some other features related to the geographies. An example gazetteer file for the US is given below.\n",
    "\n",
    "Data download URL: https://raw.githubusercontent.com/grammakov/USA-cities-and-states/master/us_cities_states_counties.csv\n",
    "\n",
    "\n",
    "We'll write a simple function which takes a word as input and returns a tuple indicating **whether the word is a city, state or a county**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State short</th>\n",
       "      <th>State full</th>\n",
       "      <th>County</th>\n",
       "      <th>City alias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Holtsville</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>SUFFOLK</td>\n",
       "      <td>Internal Revenue Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Holtsville</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>SUFFOLK</td>\n",
       "      <td>Holtsville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adjuntas</td>\n",
       "      <td>PR</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>ADJUNTAS</td>\n",
       "      <td>URB San Joaquin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adjuntas</td>\n",
       "      <td>PR</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>ADJUNTAS</td>\n",
       "      <td>Jard De Adjuntas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adjuntas</td>\n",
       "      <td>PR</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>ADJUNTAS</td>\n",
       "      <td>Colinas Del Gigante</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         City State short   State full    County                City alias\n",
       "0  Holtsville          NY     New York   SUFFOLK  Internal Revenue Service\n",
       "1  Holtsville          NY     New York   SUFFOLK                Holtsville\n",
       "2    Adjuntas          PR  Puerto Rico  ADJUNTAS           URB San Joaquin\n",
       "3    Adjuntas          PR  Puerto Rico  ADJUNTAS          Jard De Adjuntas\n",
       "4    Adjuntas          PR  Puerto Rico  ADJUNTAS       Colinas Del Gigante"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# reading a file containing list of US cities, states and counties\n",
    "us_cities = pd.read_csv(\"./data/us_cities_states_counties.csv\", sep=\"|\")\n",
    "us_cities.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# storing cities, states and counties as sets\n",
    "cities = set(us_cities['City'].str.lower())\n",
    "states = set(us_cities['State full'].str.lower())\n",
    "counties = set(us_cities['County'].str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18854\n",
      "62\n",
      "1932\n"
     ]
    }
   ],
   "source": [
    "print(len(cities))\n",
    "print(len(states))\n",
    "print(len(counties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to look up a given word in cities, states, county\n",
    "def gazetteer_lookup(word):\n",
    "    return (word in cities, word in states, word in counties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, True, True)\n",
      "(False, True, True)\n",
      "(True, False, True)\n"
     ]
    }
   ],
   "source": [
    "# sample lookups\n",
    "print(gazetteer_lookup('washington'))\n",
    "print(gazetteer_lookup('utah'))\n",
    "print(gazetteer_lookup('philadelphia'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build some **classifiers for NER**, i.e. classification models which take in each word (i.e. its features) as input and predicts its IOB label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers for NER \n",
    "\n",
    "As discussed above, IOB tagging is a **sequence classification task** - given a sequence of words and pos tags, predict the IOB label of the word. \n",
    "\n",
    "One of the main advantages of classifier based chunkers is that we can use a variety of features which we think will be strong indicators of a word's IOB tag. \n",
    "\n",
    "For e.g. if a word is a state/city name such as 'boston', it is very likely an ```B-fromloc.city_name``` or ```B-toloc.city_name```. \n",
    "\n",
    "Similarly, we can expect that the **previous word and the previous POS tag** could help predict the IOB labels of a word; that **if a word is the first or the last in the sentence** may strongly help predict the IOB label, etc. \n",
    "\n",
    "Also, in all sequence classification tasks, one can use the **predicted labels of the previous words** as features (recall that HMMs compute transition probabilities).\n",
    "\n",
    "\n",
    "The following code implements the Naive Bayes classifer which uses a variety of **word features** for classification. \n",
    "\n",
    "The function ```npchunk_features()```  takes in a sentence and  the word whose features are to be extracted (defined by its index i in the sentence) as input and returns a dictionary of word features as output. It also takes a ```history``` argument - a list of already predicted previous tags to the left of the target word, which is useful if you are using them as features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts features for the word at index i in a sentence \n",
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    \n",
    "    # the first word has both previous word and previous tag undefined\n",
    "    if i == 0:\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i-1]\n",
    "\n",
    "    # gazetteer lookup features (see section below)\n",
    "    gazetteer = gazetteer_lookup(word)\n",
    "\n",
    "    return {\"pos\": pos, \"prevpos\": prevpos, 'word':word,\n",
    "           'word_is_city': gazetteer[0],\n",
    "           'word_is_state': gazetteer[1],\n",
    "           'word_is_county': gazetteer[2]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at word features of some example sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what', 'WP'),\n",
       " ('flights', 'NNS'),\n",
       " ('leave', 'VBP'),\n",
       " ('atlanta', 'VBN'),\n",
       " ('at', 'IN'),\n",
       " ('about', 'RB'),\n",
       " ('DIGIT', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('afternoon', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('arrive', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('san', 'JJ'),\n",
       " ('francisco', 'NN')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example sentence\n",
    "sent_pos = train_pos[0]\n",
    "sent_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pos': 'WP', 'prevpos': '<START>', 'word': 'what', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'NNS', 'prevpos': 'WP', 'word': 'flights', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'VBP', 'prevpos': 'NNS', 'word': 'leave', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'VBN', 'prevpos': 'VBP', 'word': 'atlanta', 'word_is_city': True, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'IN', 'prevpos': 'VBN', 'word': 'at', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'RB', 'prevpos': 'IN', 'word': 'about', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'NNP', 'prevpos': 'RB', 'word': 'DIGIT', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'IN', 'prevpos': 'NNP', 'word': 'in', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'DT', 'prevpos': 'IN', 'word': 'the', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'NN', 'prevpos': 'DT', 'word': 'afternoon', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'CC', 'prevpos': 'NN', 'word': 'and', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'NN', 'prevpos': 'CC', 'word': 'arrive', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'IN', 'prevpos': 'NN', 'word': 'in', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'JJ', 'prevpos': 'IN', 'word': 'san', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'NN', 'prevpos': 'JJ', 'word': 'francisco', 'word_is_city': True, 'word_is_state': False, 'word_is_county': False}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# features for sentence sent_pos\n",
    "# each word's features are stored in a dict\n",
    "for i in range(len(sent_pos)):\n",
    "    print(npchunk_features(sent_pos, i, history=[]))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define two classes ```ConsecutiveNPChunkTagger``` and ```ConsecutiveNPChunker```.\n",
    "\n",
    "The ```__init__``` method of the ```ConsecutiveNPChunkTagger``` class creates the ```train_set``` which is a list of labelled training sentences. Each sentence is a list of tuples (featureset, tag) -  each tuple is the featureset (dict) of a word and its label. \n",
    "\n",
    "The list ```history``` contains the list of previously predicted IOB tags, i.e. tags to the left of the target word. We can only use IOB tags to the left of the target word since that's all the tags we have at the time of prediction.\n",
    "\n",
    "The ```__init__``` method takes in an IOB tagged list of train_sents and loops through them. It first untags the IOB tags to generate (word, pos_tag) tuples stored in ```untagged_sent```. These tuples are used to compute the word features.\n",
    "\n",
    "Then for each (word, IOB_tag) in ```tagged_sent```, it computes the word features and appends the feature dict and the tag to ```train_sents```. It further appends the IOB tag to ```history```.\n",
    " \n",
    "The ```tag()``` method simply takes in a sentence as a list of words and predicts the IOB label of each word in the sentence.\n",
    "\n",
    "The ```ConsecutiveNPChunker``` class does all the the uninteresting work of converting between tree-list-tree formats (since NLTK's builtin classifiers need the list format). It takes in a list of sentences as trees, converts each sentence to the list form, and then initialises its tagger using methods already defined in the ```ConsecutiveNPChunkTagger``` class. The ```parse``` method tags the sentence and returns it in the tree format since it is easier to print and read.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutiveNPChunkTagger(nltk.TaggerI): \n",
    "\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            # compute features for each word\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = npchunk_features(untagged_sent, i, history) \n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = npchunk_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "\n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI): \n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = [[((w,t),c) for (w,t,c) in\n",
    "                         nltk.chunk.tree2conlltags(sent)]\n",
    "                        for sent in train_sents]\n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the chunker \n",
    "chunker = ConsecutiveNPChunker(train_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  91.7%%\n",
      "    Precision:     75.3%%\n",
      "    Recall:        81.8%%\n",
      "    F-Measure:     78.4%%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the chunker\n",
    "print(chunker.evaluate(valid_trees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results have improved significantly compared to the basic unigram/bigram chunkers, and they may improve further if we create better features.\n",
    "\n",
    "For example, if the word is 'DIGIT' (numbers are labelled as 'DIGIT' in this dataset), we can have a feature which indicates that (see example below). In this dataset, 4-digit numbers are encoded as 'DIGITDIGITDIGITDIGIT'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('do', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('have', 'VB'),\n",
       " ('an', 'DT'),\n",
       " ('DIGITDIGITDIGIT', 'NNP'),\n",
       " ('flight', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('denver', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('san', 'VB'),\n",
       " ('francisco', 'NN')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of 'DIGITDIGITDIGIT'\n",
    "train_pos[1326]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some of these features and see if the performance improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts features for a given word i in a given sentence \n",
    "# history refers to the previous POS tags in the sentence\n",
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    \n",
    "    # the first word has both previous word and previous tag undefined\n",
    "    if i == 0:\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i-1]\n",
    "        \n",
    "    if i == len(sentence)-1:\n",
    "        nextword, nextpos = '<END>', '<END>'\n",
    "    else:\n",
    "        nextword, nextpos = sentence[i+1]\n",
    "\n",
    "    # gazetteer lookup features (see section below)\n",
    "    gazetteer = gazetteer_lookup(word)\n",
    "\n",
    "    # adding word_is_digit feature (boolean)\n",
    "    return {\"pos\": pos, \"prevpos\": prevpos, 'word':word, \n",
    "           'word_is_city': gazetteer[0],\n",
    "           'word_is_state': gazetteer[1],\n",
    "           'word_is_county': gazetteer[2],\n",
    "           'word_is_digit': word in 'DIGITDIGITDIGIT', \n",
    "           'nextword': nextword, \n",
    "           'nextpos': nextpos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  91.7%%\n",
      "    Precision:     75.9%%\n",
      "    Recall:        85.1%%\n",
      "    F-Measure:     80.3%%\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate the chunker \n",
    "chunker = ConsecutiveNPChunker(train_trees)\n",
    "print(chunker.evaluate(valid_trees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the most informative features of the NLTK NB classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                     pos = 'JJS'          B-cost : O      =  14237.4 : 1.0\n",
      "                    word = 'after'        B-depa : O      =   5457.6 : 1.0\n",
      "           word_is_digit = True           B-depa : O      =   5138.3 : 1.0\n",
      "                    word = 'pm'           I-depa : O      =   4549.1 : 1.0\n",
      "                    word = 'DIGIT'        B-depa : O      =   3148.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# most top-N informative features\n",
    "chunker.tagger.classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some informative features are ```pos```, ```word```, ```word_is_digit``` etc.\n",
    "\n",
    "When pos=JJS (superlative adjective e.g. *\"cheapest\"*), the ratio of probability of the labels ```B-cost:O``` is about 14237:1. Similarly, the features ```word``` and ```word_is_digit``` are very strong indicators of the labels ```B-depa``` and ```I-depa``` (departure time e.g. *\"after DIGIT pm\"*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try other classifiers that come with NLTK - let's try building a **decision tree**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "class ConsecutiveNPChunkTagger(nltk.TaggerI): \n",
    "\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            # compute features for each word\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = npchunk_features(untagged_sent, i, history) \n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = npchunk_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "\n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI): \n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = [[((w,t),c) for (w,t,c) in\n",
    "                         nltk.chunk.tree2conlltags(sent)]\n",
    "                        for sent in train_sents]\n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  94.9%%\n",
      "    Precision:     83.9%%\n",
      "    Recall:        87.2%%\n",
      "    F-Measure:     85.5%%\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate the decision tree chunker\n",
    "tree_chunker = ConsecutiveNPChunker(train_trees)\n",
    "print(tree_chunker.evaluate(valid_trees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of decision trees is much better compared to the Naive Bayes classifier. We can of course also tune the decision tree hyperparameters (maxdepth, num_leaves, min_sample_split etc.), but we'll skip that for now.\n",
    "\n",
    "Having tried some classification models, let us now try another extremely useful and popular model for sequence classification - **conditional random fields**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Random Fields (CRF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now build a CRF classifier. In sklearn, CRFs are implemented in the library ```sklearn_crfsuite```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to extract features from a given sentence. This is similar to the ```npchunk_features()``` function defined above, but we'll add some new features as well such as the **suffix** of the word (upto the last 4 characters), **prefix** (upto first 4 characters) etc.\n",
    "\n",
    "The list of features we'll extract is as follows:\n",
    "```\n",
    "{\n",
    "            'word':word,\n",
    "            'pos': pos, \n",
    "            'prevword': prevword,\n",
    "            'prevpos': prevpos,  \n",
    "            'nextword': nextword, \n",
    "            'nextpos': nextpos,\n",
    "            'word_is_city': gazetteer[0],\n",
    "            'word_is_state': gazetteer[1],\n",
    "            'word_is_county': gazetteer[2],\n",
    "            'word_is_digit': word in 'DIGITDIGITDIGIT',\n",
    "            'suff_1': suff_1,  \n",
    "            'suff_2': suff_2,  \n",
    "            'suff_3': suff_3,  \n",
    "            'suff_4': suff_4, \n",
    "            'pref_1': pref_1,  \n",
    "            'pref_2': pref_2,  \n",
    "            'pref_3': pref_3, \n",
    "            'pref_4': pref_4 \n",
    "\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from a given sentence\n",
    "def word_features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    pos = sent[i][1]\n",
    "    \n",
    "    # first word\n",
    "    if i==0:\n",
    "        prevword = '<START>'\n",
    "        prevpos = '<START>'\n",
    "    else:\n",
    "        prevword = sent[i-1][0]\n",
    "        prevpos = sent[i-1][1]\n",
    "    \n",
    "    # last word\n",
    "    if i == len(sent)-1:\n",
    "        nextword = '<END>'\n",
    "        nextpos = '<END>'\n",
    "    else:\n",
    "        nextword = sent[i+1][0]\n",
    "        nextpos = sent[i+1][1]\n",
    "    \n",
    "    # word is in gazetteer\n",
    "    gazetteer = gazetteer_lookup(word)\n",
    "    \n",
    "    # suffixes and prefixes\n",
    "    pref_1, pref_2, pref_3, pref_4 = word[:1], word[:2], word[:3], word[:4]\n",
    "    suff_1, suff_2, suff_3, suff_4 = word[-1:], word[-2:], word[-3:], word[-4:]\n",
    "    \n",
    "    return {'word':word,\n",
    "            'pos': pos, \n",
    "            'prevword': prevword,\n",
    "            'prevpos': prevpos,  \n",
    "            'nextword': nextword, \n",
    "            'nextpos': nextpos,\n",
    "            'word_is_city': gazetteer[0],\n",
    "            'word_is_state': gazetteer[1],\n",
    "            'word_is_county': gazetteer[2],\n",
    "            'word_is_digit': word in 'DIGITDIGITDIGIT',\n",
    "            'suff_1': suff_1,  \n",
    "            'suff_2': suff_2,  \n",
    "            'suff_3': suff_3,  \n",
    "            'suff_4': suff_4, \n",
    "            'pref_1': pref_1,  \n",
    "            'pref_2': pref_2,  \n",
    "            'pref_3': pref_3, \n",
    "            'pref_4': pref_4 }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'atlanta',\n",
       " 'pos': 'VBN',\n",
       " 'prevword': 'leave',\n",
       " 'prevpos': 'VBP',\n",
       " 'nextword': 'at',\n",
       " 'nextpos': 'IN',\n",
       " 'word_is_city': True,\n",
       " 'word_is_state': False,\n",
       " 'word_is_county': False,\n",
       " 'word_is_digit': False,\n",
       " 'suff_1': 'a',\n",
       " 'suff_2': 'ta',\n",
       " 'suff_3': 'nta',\n",
       " 'suff_4': 'anta',\n",
       " 'pref_1': 'a',\n",
       " 'pref_2': 'at',\n",
       " 'pref_3': 'atl',\n",
       " 'pref_4': 'atla'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example features: third word of the first training sentence\n",
    "word_features(train_labels[0], i=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write some functions to convert the training, validation and test datasets to the format required by the sklearn CRF classifier. The structure of the training sentences is as given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what', 'WP', 'O'),\n",
       " ('flights', 'NNS', 'O'),\n",
       " ('leave', 'VBP', 'O'),\n",
       " ('atlanta', 'VBN', 'B-fromloc.city_name'),\n",
       " ('at', 'IN', 'O'),\n",
       " ('about', 'RB', 'B-depart_time.time_relative'),\n",
       " ('DIGIT', 'NNP', 'B-depart_time.time'),\n",
       " ('in', 'IN', 'O'),\n",
       " ('the', 'DT', 'O'),\n",
       " ('afternoon', 'NN', 'B-depart_time.period_of_day'),\n",
       " ('and', 'CC', 'O'),\n",
       " ('arrive', 'NN', 'O'),\n",
       " ('in', 'IN', 'O'),\n",
       " ('san', 'JJ', 'B-toloc.city_name'),\n",
       " ('francisco', 'NN', 'I-toloc.city_name')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# structure of train/validation data\n",
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define three functions below - ```sent2features()``` creates word features for each word in a given sentence while ```sent2labels``` extracts the label of each word in a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a few more functions to extract featrues, labels, words from sentences\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word_features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's convert the training data into a standard format - ```X_train``` is a list of sentences, where each sentence is a list of word_features (dicts). The list ```y_train``` is a list of sentences, each sentence further being a list of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    " # create training, validation and test sets\n",
    "X_train = [sent2features(s) for s in train_labels]\n",
    "y_train = [sent2labels(s) for s in train_labels]\n",
    "\n",
    "X_valid = [sent2features(s) for s in valid_labels]\n",
    "y_valid = [sent2labels(s) for s in valid_labels]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_labels]\n",
    "y_test = [sent2labels(s) for s in test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'what',\n",
       "  'pos': 'WP',\n",
       "  'prevword': '<START>',\n",
       "  'prevpos': '<START>',\n",
       "  'nextword': 'flights',\n",
       "  'nextpos': 'NNS',\n",
       "  'word_is_city': False,\n",
       "  'word_is_state': False,\n",
       "  'word_is_county': False,\n",
       "  'word_is_digit': False,\n",
       "  'suff_1': 't',\n",
       "  'suff_2': 'at',\n",
       "  'suff_3': 'hat',\n",
       "  'suff_4': 'what',\n",
       "  'pref_1': 'w',\n",
       "  'pref_2': 'wh',\n",
       "  'pref_3': 'wha',\n",
       "  'pref_4': 'what'},\n",
       " {'word': 'flights',\n",
       "  'pos': 'NNS',\n",
       "  'prevword': 'what',\n",
       "  'prevpos': 'WP',\n",
       "  'nextword': 'leave',\n",
       "  'nextpos': 'VBP',\n",
       "  'word_is_city': False,\n",
       "  'word_is_state': False,\n",
       "  'word_is_county': False,\n",
       "  'word_is_digit': False,\n",
       "  'suff_1': 's',\n",
       "  'suff_2': 'ts',\n",
       "  'suff_3': 'hts',\n",
       "  'suff_4': 'ghts',\n",
       "  'pref_1': 'f',\n",
       "  'pref_2': 'fl',\n",
       "  'pref_3': 'fli',\n",
       "  'pref_4': 'flig'}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train is a list of sentences within which each feature has a corresponding dict of features\n",
    "# first few words (each word has a feature dict) of the first sentence in X_train\n",
    "X_train[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-fromloc.city_name',\n",
       " 'O',\n",
       " 'B-depart_time.time_relative',\n",
       " 'B-depart_time.time',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-depart_time.period_of_day',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-toloc.city_name',\n",
       " 'I-toloc.city_name']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels of the first sentence\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now fit a CRF with arbitrary hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.01, c2=0.1,\n",
       "    keep_tempfiles=None, max_iterations=100)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting crf with arbitrary hyperparameters\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.01,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now remove the label 'O' from the list of labels (can be accessed through ```crf.classes_```). This is because we'll only measure the metrics for other less frequent classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-fromloc.city_name',\n",
       " 'B-depart_time.time_relative',\n",
       " 'B-depart_time.time',\n",
       " 'B-depart_time.period_of_day',\n",
       " 'B-toloc.city_name']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove 'O' from the labels\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make predictions using the validation data and evaluate model performance. The metric we'll use for evaluation is ```flat_f1_score``` which is a weighed average of each class' f1-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9372302908496231"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions \n",
    "y_pred = crf.predict(X_valid)\n",
    "metrics.flat_f1_score(y_valid, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall score does not give a detailed picture of how respective classes are performing. For e.g. we'd want to know how many ```fromloc.city_name``` and ```toloc.city_name``` are correctly identified. \n",
    "\n",
    "We can see the class-wise evaluation metrics using sklearn's ```classification_report```. The metrics precision, recall and f1-score are as usual, while support represents the number of instances of the respective class in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              precision    recall  f1-score   support\n",
      "\n",
      "             B-aircraft_code      1.000     1.000     1.000         3\n",
      "              B-airline_code      1.000     0.963     0.981        27\n",
      "              B-airline_name      1.000     0.993     0.996       139\n",
      "              I-airline_name      1.000     0.975     0.987        80\n",
      "              B-airport_code      1.000     0.800     0.889         5\n",
      "              B-airport_name      0.600     0.429     0.500         7\n",
      "              I-airport_name      0.600     0.333     0.429         9\n",
      " B-arrive_date.date_relative      1.000     1.000     1.000         1\n",
      "      B-arrive_date.day_name      0.400     0.143     0.211        14\n",
      "    B-arrive_date.day_number      0.600     0.353     0.444        17\n",
      "    I-arrive_date.day_number      0.000     0.000     0.000         2\n",
      "    B-arrive_date.month_name      0.600     0.353     0.444        17\n",
      "      B-arrive_time.end_time      1.000     0.429     0.600         7\n",
      "      I-arrive_time.end_time      1.000     0.500     0.667         6\n",
      "    B-arrive_time.period_mod      0.000     0.000     0.000         1\n",
      " B-arrive_time.period_of_day      1.000     0.111     0.200         9\n",
      " I-arrive_time.period_of_day      0.000     0.000     0.000         0\n",
      "    B-arrive_time.start_time      1.000     0.571     0.727         7\n",
      "    I-arrive_time.start_time      1.000     1.000     1.000         3\n",
      "          B-arrive_time.time      0.829     0.791     0.810        43\n",
      "          I-arrive_time.time      0.816     0.886     0.849        35\n",
      " B-arrive_time.time_relative      0.829     0.784     0.806        37\n",
      "                 B-city_name      0.680     0.395     0.500        43\n",
      "                 I-city_name      0.400     0.571     0.471         7\n",
      "                B-class_type      0.979     1.000     0.989        47\n",
      "                I-class_type      1.000     1.000     1.000        38\n",
      "                   B-connect      0.889     0.889     0.889         9\n",
      "             B-cost_relative      1.000     1.000     1.000        60\n",
      "             I-cost_relative      1.000     1.000     1.000         4\n",
      "                  B-day_name      0.000     0.000     0.000         0\n",
      "                B-day_number      0.000     0.000     0.000         0\n",
      "                 B-days_code      0.000     0.000     0.000         0\n",
      " B-depart_date.date_relative      0.917     0.846     0.880        13\n",
      "      B-depart_date.day_name      0.924     0.981     0.952       161\n",
      "    B-depart_date.day_number      0.867     0.951     0.907        82\n",
      "    I-depart_date.day_number      0.885     1.000     0.939        23\n",
      "    B-depart_date.month_name      0.855     0.947     0.899        75\n",
      "B-depart_date.today_relative      0.920     1.000     0.958        23\n",
      "I-depart_date.today_relative      0.000     0.000     0.000         0\n",
      "          B-depart_date.year      1.000     1.000     1.000         8\n",
      "      B-depart_time.end_time      0.400     1.000     0.571         2\n",
      "      I-depart_time.end_time      0.333     1.000     0.500         1\n",
      "    B-depart_time.period_mod      0.875     1.000     0.933        14\n",
      " B-depart_time.period_of_day      0.936     0.971     0.953       136\n",
      " I-depart_time.period_of_day      0.000     0.000     0.000         0\n",
      "    B-depart_time.start_time      0.400     1.000     0.571         2\n",
      "    I-depart_time.start_time      1.000     1.000     1.000         1\n",
      "          B-depart_time.time      0.838     0.905     0.870        74\n",
      "          I-depart_time.time      0.907     0.875     0.891        56\n",
      " B-depart_time.time_relative      0.846     0.902     0.873        61\n",
      " I-depart_time.time_relative      0.000     0.000     0.000         0\n",
      "                   B-economy      1.000     1.000     1.000         9\n",
      "                   I-economy      1.000     1.000     1.000         2\n",
      "               B-fare_amount      1.000     1.000     1.000         5\n",
      "               I-fare_amount      1.000     1.000     1.000         5\n",
      "           B-fare_basis_code      1.000     0.938     0.968        16\n",
      "           I-fare_basis_code      0.000     0.000     0.000         0\n",
      "               B-flight_days      1.000     1.000     1.000         4\n",
      "                B-flight_mod      0.971     0.943     0.957        70\n",
      "                I-flight_mod      0.000     0.000     0.000         2\n",
      "             B-flight_number      1.000     0.905     0.950        21\n",
      "               B-flight_stop      0.971     0.895     0.932        38\n",
      "               I-flight_stop      1.000     0.333     0.500         6\n",
      "               B-flight_time      0.909     0.833     0.870        12\n",
      "               I-flight_time      0.833     0.833     0.833         6\n",
      "      B-fromloc.airport_code      1.000     0.800     0.889         5\n",
      "      B-fromloc.airport_name      0.842     0.800     0.821        20\n",
      "      I-fromloc.airport_name      0.909     0.690     0.784        29\n",
      "         B-fromloc.city_name      0.988     0.983     0.986       868\n",
      "         I-fromloc.city_name      0.971     0.993     0.982       134\n",
      "        B-fromloc.state_code      1.000     0.929     0.963        14\n",
      "        B-fromloc.state_name      1.000     1.000     1.000        12\n",
      "        I-fromloc.state_name      1.000     1.000     1.000         3\n",
      "                      B-meal      1.000     1.000     1.000        11\n",
      "                 B-meal_code      0.000     0.000     0.000         0\n",
      "                 I-meal_code      0.000     0.000     0.000         0\n",
      "          B-meal_description      1.000     1.000     1.000        12\n",
      "          I-meal_description      0.000     0.000     0.000         0\n",
      "                       B-mod      0.400     1.000     0.571         2\n",
      "                B-month_name      0.000     0.000     0.000         0\n",
      "                        B-or      1.000     0.950     0.974        20\n",
      "             B-period_of_day      0.000     0.000     0.000         0\n",
      "          B-restriction_code      1.000     1.000     1.000         6\n",
      "          I-restriction_code      1.000     1.000     1.000         1\n",
      " B-return_date.date_relative      0.667     1.000     0.800         2\n",
      " I-return_date.date_relative      0.000     0.000     0.000         0\n",
      "      B-return_date.day_name      0.000     0.000     0.000         0\n",
      "    B-return_date.day_number      0.000     0.000     0.000         1\n",
      "    B-return_date.month_name      0.000     0.000     0.000         1\n",
      "B-return_date.today_relative      0.000     0.000     0.000         0\n",
      "I-return_date.today_relative      0.000     0.000     0.000         0\n",
      "    B-return_time.period_mod      1.000     1.000     1.000         1\n",
      " B-return_time.period_of_day      1.000     0.500     0.667         2\n",
      "                B-round_trip      1.000     1.000     1.000        62\n",
      "                I-round_trip      1.000     1.000     1.000        61\n",
      "                B-state_code      0.333     1.000     0.500         1\n",
      "                B-state_name      0.000     0.000     0.000         0\n",
      "      B-stoploc.airport_name      0.000     0.000     0.000         0\n",
      "         B-stoploc.city_name      0.677     0.815     0.739        54\n",
      "         I-stoploc.city_name      1.000     0.500     0.667         6\n",
      "        B-stoploc.state_code      0.000     0.000     0.000         1\n",
      "                      B-time      0.000     0.000     0.000         0\n",
      "                      I-time      0.000     0.000     0.000         0\n",
      "             B-time_relative      0.000     0.000     0.000         0\n",
      "            B-today_relative      0.000     0.000     0.000         0\n",
      "            I-today_relative      0.000     0.000     0.000         0\n",
      "        B-toloc.airport_code      1.000     1.000     1.000         4\n",
      "        B-toloc.airport_name      0.750     0.818     0.783        11\n",
      "        I-toloc.airport_name      0.632     0.923     0.750        13\n",
      "           B-toloc.city_name      0.970     0.983     0.976       860\n",
      "           I-toloc.city_name      0.983     0.988     0.986       241\n",
      "        B-toloc.country_name      0.000     0.000     0.000         0\n",
      "          B-toloc.state_code      1.000     1.000     1.000        19\n",
      "          B-toloc.state_name      1.000     1.000     1.000        22\n",
      "          I-toloc.state_name      1.000     1.000     1.000         4\n",
      "            B-transport_type      1.000     0.778     0.875         9\n",
      "            I-transport_type      1.000     1.000     1.000         4\n",
      "\n",
      "                   micro avg      0.943     0.939     0.941      4121\n",
      "                   macro avg      0.649     0.631     0.622      4121\n",
      "                weighted avg      0.942     0.939     0.937      4121\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# class-wise scores\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_valid, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report tells us that:\n",
    "- The overall f1-score (a weighted average of overall precision and recall) is 93.7%\n",
    "- The *important* classes, i.e. those with high frequency such as ```B-fromloc.city_name```, ```B-toloc.city_name```, ```B-depart_date.day_name``` etc. are performing well\n",
    "\n",
    "Let's now try tuning the hyperparameters of CRF using grid search CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "\n",
    "# define fixed parameters and parameters to search\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "# parameters to tune\n",
    "params_space = {\n",
    "    'c1': [0.01, 0.1, 1],\n",
    "    'c2': [0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = scorers.make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed:  6.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=CRF(algorithm='lbfgs', all_possible_transitions=True,\n",
       "                           keep_tempfiles=None, max_iterations=100),\n",
       "             n_jobs=-1, param_grid={'c1': [0.01, 0.1, 1], 'c2': [0.01, 0.1, 1]},\n",
       "             return_train_score=True,\n",
       "             scoring=make_scorer(flat_f1_score, average=weighted, labels=['B-fromloc.city_name', 'B-depart_time.time_relative', 'B-depart_time.time', 'B-depart_time....name', 'B-month_name', 'B-day_number', 'B-depart_date.year', 'I-arrive_date.day_number', 'B-period_of_day', 'B-time_relative', 'B-time', 'I-time', 'I-flight_mod', 'B-return_date.day_name', 'I-fare_basis_code', 'I-depart_date.today_relative', 'B-stoploc.state_code', 'I-arrive_time.start_time', 'B-state_name', 'I-today_relative', 'B-stoploc.airport_name']),\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate a GridSearchCV object\n",
    "rs = GridSearchCV(crf, \n",
    "                  params_space,\n",
    "                  cv=3,\n",
    "                  verbose=1,\n",
    "                  n_jobs=-1,\n",
    "                  scoring=f1_scorer, \n",
    "                  return_train_score=True)\n",
    "# fit\n",
    "rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_c1</th>\n",
       "      <th>param_c2</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83.163710</td>\n",
       "      <td>7.842350</td>\n",
       "      <td>0.494796</td>\n",
       "      <td>0.026779</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'c1': 0.01, 'c2': 0.01}</td>\n",
       "      <td>0.930426</td>\n",
       "      <td>0.922370</td>\n",
       "      <td>0.934504</td>\n",
       "      <td>0.929100</td>\n",
       "      <td>0.005042</td>\n",
       "      <td>4</td>\n",
       "      <td>0.978361</td>\n",
       "      <td>0.980400</td>\n",
       "      <td>0.979812</td>\n",
       "      <td>0.979525</td>\n",
       "      <td>0.000857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82.985665</td>\n",
       "      <td>8.374674</td>\n",
       "      <td>0.523766</td>\n",
       "      <td>0.032698</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'c1': 0.01, 'c2': 0.1}</td>\n",
       "      <td>0.930870</td>\n",
       "      <td>0.926590</td>\n",
       "      <td>0.934779</td>\n",
       "      <td>0.930746</td>\n",
       "      <td>0.003344</td>\n",
       "      <td>2</td>\n",
       "      <td>0.975267</td>\n",
       "      <td>0.977576</td>\n",
       "      <td>0.977806</td>\n",
       "      <td>0.976883</td>\n",
       "      <td>0.001147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83.350375</td>\n",
       "      <td>8.292046</td>\n",
       "      <td>0.523536</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>{'c1': 0.01, 'c2': 1}</td>\n",
       "      <td>0.920278</td>\n",
       "      <td>0.913100</td>\n",
       "      <td>0.918695</td>\n",
       "      <td>0.917358</td>\n",
       "      <td>0.003079</td>\n",
       "      <td>7</td>\n",
       "      <td>0.950655</td>\n",
       "      <td>0.949025</td>\n",
       "      <td>0.949209</td>\n",
       "      <td>0.949630</td>\n",
       "      <td>0.000729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83.290344</td>\n",
       "      <td>8.584179</td>\n",
       "      <td>0.459129</td>\n",
       "      <td>0.019482</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'c1': 0.1, 'c2': 0.01}</td>\n",
       "      <td>0.931663</td>\n",
       "      <td>0.923990</td>\n",
       "      <td>0.935359</td>\n",
       "      <td>0.930338</td>\n",
       "      <td>0.004735</td>\n",
       "      <td>3</td>\n",
       "      <td>0.978314</td>\n",
       "      <td>0.979166</td>\n",
       "      <td>0.979282</td>\n",
       "      <td>0.978921</td>\n",
       "      <td>0.000432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81.965045</td>\n",
       "      <td>8.081670</td>\n",
       "      <td>0.464700</td>\n",
       "      <td>0.018835</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'c1': 0.1, 'c2': 0.1}</td>\n",
       "      <td>0.932268</td>\n",
       "      <td>0.926484</td>\n",
       "      <td>0.936914</td>\n",
       "      <td>0.931889</td>\n",
       "      <td>0.004267</td>\n",
       "      <td>1</td>\n",
       "      <td>0.973995</td>\n",
       "      <td>0.975931</td>\n",
       "      <td>0.976143</td>\n",
       "      <td>0.975356</td>\n",
       "      <td>0.000967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>82.582656</td>\n",
       "      <td>7.990042</td>\n",
       "      <td>0.497768</td>\n",
       "      <td>0.013172</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'c1': 0.1, 'c2': 1}</td>\n",
       "      <td>0.917787</td>\n",
       "      <td>0.912097</td>\n",
       "      <td>0.918497</td>\n",
       "      <td>0.916127</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>8</td>\n",
       "      <td>0.948426</td>\n",
       "      <td>0.947578</td>\n",
       "      <td>0.947646</td>\n",
       "      <td>0.947883</td>\n",
       "      <td>0.000385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>81.303436</td>\n",
       "      <td>8.136291</td>\n",
       "      <td>0.425250</td>\n",
       "      <td>0.009548</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'c1': 1, 'c2': 0.01}</td>\n",
       "      <td>0.917660</td>\n",
       "      <td>0.920771</td>\n",
       "      <td>0.922260</td>\n",
       "      <td>0.920230</td>\n",
       "      <td>0.001916</td>\n",
       "      <td>5</td>\n",
       "      <td>0.949051</td>\n",
       "      <td>0.950010</td>\n",
       "      <td>0.949515</td>\n",
       "      <td>0.949525</td>\n",
       "      <td>0.000392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>81.365551</td>\n",
       "      <td>8.469283</td>\n",
       "      <td>0.405673</td>\n",
       "      <td>0.015251</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'c1': 1, 'c2': 0.1}</td>\n",
       "      <td>0.914650</td>\n",
       "      <td>0.918630</td>\n",
       "      <td>0.922353</td>\n",
       "      <td>0.918544</td>\n",
       "      <td>0.003145</td>\n",
       "      <td>6</td>\n",
       "      <td>0.945374</td>\n",
       "      <td>0.948256</td>\n",
       "      <td>0.944271</td>\n",
       "      <td>0.945967</td>\n",
       "      <td>0.001680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>72.841418</td>\n",
       "      <td>3.644624</td>\n",
       "      <td>0.383493</td>\n",
       "      <td>0.013796</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'c1': 1, 'c2': 1}</td>\n",
       "      <td>0.905066</td>\n",
       "      <td>0.905441</td>\n",
       "      <td>0.901998</td>\n",
       "      <td>0.904168</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>9</td>\n",
       "      <td>0.928574</td>\n",
       "      <td>0.931405</td>\n",
       "      <td>0.925755</td>\n",
       "      <td>0.928578</td>\n",
       "      <td>0.002306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_c1  \\\n",
       "0      83.163710      7.842350         0.494796        0.026779     0.01   \n",
       "1      82.985665      8.374674         0.523766        0.032698     0.01   \n",
       "2      83.350375      8.292046         0.523536        0.017986     0.01   \n",
       "3      83.290344      8.584179         0.459129        0.019482      0.1   \n",
       "4      81.965045      8.081670         0.464700        0.018835      0.1   \n",
       "5      82.582656      7.990042         0.497768        0.013172      0.1   \n",
       "6      81.303436      8.136291         0.425250        0.009548        1   \n",
       "7      81.365551      8.469283         0.405673        0.015251        1   \n",
       "8      72.841418      3.644624         0.383493        0.013796        1   \n",
       "\n",
       "  param_c2                    params  split0_test_score  split1_test_score  \\\n",
       "0     0.01  {'c1': 0.01, 'c2': 0.01}           0.930426           0.922370   \n",
       "1      0.1   {'c1': 0.01, 'c2': 0.1}           0.930870           0.926590   \n",
       "2        1     {'c1': 0.01, 'c2': 1}           0.920278           0.913100   \n",
       "3     0.01   {'c1': 0.1, 'c2': 0.01}           0.931663           0.923990   \n",
       "4      0.1    {'c1': 0.1, 'c2': 0.1}           0.932268           0.926484   \n",
       "5        1      {'c1': 0.1, 'c2': 1}           0.917787           0.912097   \n",
       "6     0.01     {'c1': 1, 'c2': 0.01}           0.917660           0.920771   \n",
       "7      0.1      {'c1': 1, 'c2': 0.1}           0.914650           0.918630   \n",
       "8        1        {'c1': 1, 'c2': 1}           0.905066           0.905441   \n",
       "\n",
       "   split2_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "0           0.934504         0.929100        0.005042                4   \n",
       "1           0.934779         0.930746        0.003344                2   \n",
       "2           0.918695         0.917358        0.003079                7   \n",
       "3           0.935359         0.930338        0.004735                3   \n",
       "4           0.936914         0.931889        0.004267                1   \n",
       "5           0.918497         0.916127        0.002864                8   \n",
       "6           0.922260         0.920230        0.001916                5   \n",
       "7           0.922353         0.918544        0.003145                6   \n",
       "8           0.901998         0.904168        0.001542                9   \n",
       "\n",
       "   split0_train_score  split1_train_score  split2_train_score  \\\n",
       "0            0.978361            0.980400            0.979812   \n",
       "1            0.975267            0.977576            0.977806   \n",
       "2            0.950655            0.949025            0.949209   \n",
       "3            0.978314            0.979166            0.979282   \n",
       "4            0.973995            0.975931            0.976143   \n",
       "5            0.948426            0.947578            0.947646   \n",
       "6            0.949051            0.950010            0.949515   \n",
       "7            0.945374            0.948256            0.944271   \n",
       "8            0.928574            0.931405            0.925755   \n",
       "\n",
       "   mean_train_score  std_train_score  \n",
       "0          0.979525         0.000857  \n",
       "1          0.976883         0.001147  \n",
       "2          0.949630         0.000729  \n",
       "3          0.978921         0.000432  \n",
       "4          0.975356         0.000967  \n",
       "5          0.947883         0.000385  \n",
       "6          0.949525         0.000392  \n",
       "7          0.945967         0.001680  \n",
       "8          0.928578         0.002306  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store CV results in a DF\n",
    "cv_results = pd.DataFrame(rs.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time',\n",
       "       'param_c1', 'param_c2', 'params', 'split0_test_score',\n",
       "       'split1_test_score', 'split2_test_score', 'mean_test_score',\n",
       "       'std_test_score', 'rank_test_score', 'split0_train_score',\n",
       "       'split1_train_score', 'split2_train_score', 'mean_train_score',\n",
       "       'std_train_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7wAAAGHCAYAAACTYV5wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABVw0lEQVR4nO3deZhcZZX48e/pJense0jIDgTISpZmE9lEGEQFQVFQFHDBDR3HkRFHf+qgjjrjMA4uKIwQQAEBFRkBUZDFBZAEQtghQEIWlpCwZCfdeX9/3Ep3daeT7iRdXV1V38/z3Kerbr331qm+T0761Ln3vZFSQpIkSZKkclNV7AAkSZIkSSoEC15JkiRJUlmy4JUkSZIklSULXkmSJElSWbLglSRJkiSVJQteSZIkSVJZsuCVJEmSJJUlC151exHx9oj4S0S8GhEvRMT/RkS/ndhPz4i4JCJez+3n8+2M/6fcuNdz2/XMe+0bEfFQRDRExNd34mNJ0nYVI/dFxNSIuCUiXo6ItGufQJJ2XifmwPdGxN8iYl1E3FGAUNXNWfCqFAwAvgnsDkwCRgH/uRP7+TowERgHHAn8S0Qc29bAiPgH4FzgqNz4PYB/yxuyEPgX4MadiEOSOqLLcx+wCbgG+MhOvI8kdabOyoGrgO8D3+m0yFRSLHjVrUTEmIj4dUSsiIiVEfHDlNKVKaXfp5TWpZReAS4GDtmJ3Z8OfCOl9EpK6bHcfs7YztifpZQeyb3nN/LHppQuSyndDKzeiTgkqYXukvtSSk+klH4GPLKzn0WSdlQhc2BK6daU0jXA8k4PXCXBglfdRkRUA78DFgPjyb7Ju7qNoYeR98dYRPw4d7pLW8uC3JhBwEjgwbz9PAhM2UY4U9oYu1tEDNm5TydJbetmuU+SulQhc6AEUFPsAKQ8B5CdtnJOSqkht+4v+QMi4miybsWBW9allD4FfKqdfffN/Xwtb91rwLauBenbxlhy41e2816StCO6U+6TpK5WyBwo2eFVtzIGWJyX7FqIiIOAK4H3pJSe3MF9r8n97J+3rj/bPiV5TRtj2c54SdpZ3Sn3SVJXK2QOlCx41a0sAcZGxFZnHkTETOAG4MMppdtavfaTiFizjeURgNy1H88D++Vtuh/bvk7tkTbGvphSsrsrqbN1p9wnSV2tYDlQAoiUvOuAuofcNRz3A38EvgY0ArPJTr+7DfhsSumXu7D/7wAHA+8CdgNuB85MKf2+jbHHAnOAt5BNcvBr4O8ppXNzr9cC1cAlwDNkswhuSik17mx8kipTN8t9AfQkm5n+EaAXkFJKG3f2/SVpe7ogB1YDtWST9b0fOAZoTClt2rXIVSoseNWtRMRY4ALgUCCRncLSj+y6jXV5QxenlHZo0pXcfXQvBN4DrAe+m1I6P+99HwUmp5Sey637PPBFsj/4fgV8YssffRExJxdTvjNTSnN2JCZJgu6T+yJiPPBsq10sTimN39HPJEkdVeAceAZwaavVl6WUztjZeFVaLHglSZIkSWXJa3glSZIkSWWpoAVvRFwSES9FxMPbeD0i4oKIWBgRCyJiVt5rp0fEU7nl9Lz1syPiodw2F+SuN5KkojHXSaoE5jpJpajQHd45wLHbef1twMTcchbZNUZExGCyi9YPJLs319ciYlBumwuBj+Vtt739S1JXmIO5TlL5m4O5TlKJKWjBm1K6C1i1nSEnAJenzD3AwIgYCfwD8MeU0qrcLRX+CBybe61/SumelF18fDnZrJOSVDTmOkmVwFwnqRQV+xreUWT33tpiaW7d9tYvbWO9JHVn5jpJlcBcJ6nb2eoGz+UiIs4iO52GPn36zN53332LHJGk7mTevHkvp5SGFTuOXWWuk9Secsh35jpJ7dlWrit2wbsMGJP3fHRu3TLgiFbr78itH93G+K2klC4CLgKor69Pc+fO7ayYJZWBiFjchW9nrpNUNF2Y78x1kopmW7mu2Kc03wB8KDer30HAayml54FbgGMiYlBuUoNjgFtyr70eEQflZvH7EPDbokUvSR1jrpNUCcx1krqdgnZ4I+Iqsm/0hkbEUrIZ+moBUko/AW4CjgMWAuuAM3OvrYqIbwD35XZ1XkppyyQJnyKbJbAXcHNukaSiMddJqgTmOkmlKLJJ8cqbp75Iai0i5qWU6osdR2cy10lqS7nlO3OdpLZsK9cV+xreotm0aRNLly5lw4YNxQ5Fu6Curo7Ro0dTW1tb7FCkbslcVx7MddL2mevKg7lOhVCxBe/SpUvp168f48ePJ7tsRKUmpcTKlStZunQpEyZMKHY4Urdkrit95jqpfea60meuU6EUe9KqotmwYQNDhgwxKZawiGDIkCF+mytth7mu9JnrpPaZ60qfuU6FUrEFL2BSLAMeQ6l9/jspfR5DqX3+Oyl9HkMVQkUXvKWmb9++ACxfvpz3vOc9bY454ogjaG8ih+9///usW7eu6flxxx3Hq6++2mlxStKuMNdJqgTmOqlrWPCWoN13353rrrtup7dvnRhvuukmBg4c2AmRda7GxsZihyCpiMx1kiqBuU4qLAveIjn33HP50Y9+1PT861//Ot/73vdYs2YNRx11FLNmzWLatGn89rdb33990aJFTJ06FYD169dzyimnMGnSJE488UTWr1/fNO6Tn/wk9fX1TJkyha997WsAXHDBBSxfvpwjjzySI488EoDx48fz8ssvA3D++eczdepUpk6dyve///2m95s0aRIf+9jHmDJlCsccc0yL99ni2muvZerUqey3334cdthhQJbcvvCFLzB16lSmT5/OD37wAwBuu+02Zs6cybRp0/jwhz/Mxo0bm2L54he/yKxZs7j22mv5wx/+wMEHH8ysWbM4+eSTWbNmzS793iV1LXOduU6qBOY6c526r4qdpTnfv/3fIzy6/PVO3efk3fvztXdO2ebr73vf+/jc5z7Hpz/9aQCuueYabrnlFurq6vjNb35D//79efnllznooIM4/vjjt3lNw4UXXkjv3r157LHHWLBgAbNmzWp67Vvf+haDBw+msbGRo446igULFvDZz36W888/n9tvv52hQ4e22Ne8efO49NJLuffee0kpceCBB3L44YczaNAgnnrqKa666iouvvhi3vve9/KrX/2K0047rcX25513HrfccgujRo1qOpXmoosuYtGiRcyfP5+amhpWrVrFhg0bOOOMM7jtttvYe++9+dCHPsSFF17I5z73OQCGDBnC/fffz8svv8xJJ53ErbfeSp8+ffjud7/L+eefz1e/+tUdPRySMNdtYa6Typu5LmOukzJ2eItk5syZvPTSSyxfvpwHH3yQQYMGMWbMGFJK/Ou//ivTp0/nrW99K8uWLePFF1/c5n7uuuuupgQ1ffp0pk+f3vTaNddcw6xZs5g5cyaPPPIIjz766HZj+stf/sKJJ55Inz596Nu3LyeddBJ//vOfAZgwYQIzZswAYPbs2SxatGir7Q855BDOOOMMLr744qbTVm699VY+/vGPU1OTfbcyePBgnnjiCSZMmMDee+8NwOmnn85dd93VtJ/3ve99ANxzzz08+uijHHLIIcyYMYPLLruMxYsXb/czSOpezHXmOqkSmOvMdeq+7PDCdr+xK6STTz6Z6667jhdeeKEpGfziF79gxYoVzJs3j9raWsaPH79T07M/++yzfO973+O+++5j0KBBnHHGGbs0zXvPnj2bHldXV7d56stPfvIT7r33Xm688UZmz57NvHnzduq9+vTpA2T3Yzv66KO56qqrdi5oSS2Y69pnrpNKn7mufeY6VRI7vEX0vve9j6uvvprrrruOk08+GYDXXnuN4cOHU1tby+23397uN1+HHXYYV155JQAPP/wwCxYsAOD111+nT58+DBgwgBdffJGbb765aZt+/fqxevXqrfZ16KGHcv3117Nu3TrWrl3Lb37zGw499NAOf56nn36aAw88kPPOO49hw4axZMkSjj76aH7605/S0NAAwKpVq9hnn31YtGgRCxcuBOCKK67g8MMP32p/Bx10EH/961+bxq1du5Ynn3yyw/FI6h7MdeY6qRKY68x16p7s8BbRlClTWL16NaNGjWLkyJEAfOADH+Cd73wn06ZNo76+nn333Xe7+/jkJz/JmWeeyaRJk5g0aRKzZ88GYL/99mPmzJnsu+++jBkzhkMOOaRpm7POOotjjz2W3Xffndtvv71p/axZszjjjDM44IADAPjoRz/KzJkz2zzNpS3nnHMOTz31FCkljjrqKPbbbz+mTp3Kk08+yfTp06mtreVjH/sYZ599Npdeeiknn3wyDQ0N7L///nziE5/Yan/Dhg1jzpw5nHrqqU2TH3zzm99sOmVGUmkw15nrpEpgrjPXqXuKlFKxYyi4+vr61PoeZo899hiTJk0qUkTqTB5L7YyImJdSqi92HJ3JXFfePJbaWeWW78x15c1jqZ21rVznKc2SJEmSpLJkwStJkiRJKksWvJIkSZKksmTBK0mSJEkqSxa8kiRJkqSyZMErSZIkSSpLFrxF8uqrr/LjH/94p7Y97rjjePXVVzs3IEkqAHOdpEpgrpO6LwveItleYmxoaNjutjfddBMDBw4sQFQd09jYWLT3llRazHWSKoG5Tuq+LHiL5Nxzz+Xpp59mxowZnHPOOdxxxx0ceuihHH/88UyePBmAd73rXcyePZspU6Zw0UUXNW07fvx4Xn75ZRYtWsSkSZP42Mc+xpQpUzjmmGNYv379Vu917bXXMnXqVPbbbz8OO+wwIEtuX/jCF5g6dSrTp0/nBz/4AQC33XYbM2fOZNq0aXz4wx9m48aNTe/5xS9+kVmzZnHttdfyhz/8gYMPPphZs2Zx8skns2bNmkL/yiSVIHOdpEpgrpO6r5piB9At3HwuvPBQ5+5zxDR423e2+fJ3vvMdHn74YebPnw/AHXfcwf3338/DDz/MhAkTALjkkksYPHgw69evZ//99+fd7343Q4YMabGfp556iquuuoqLL76Y9773vfzqV7/itNNOazHmvPPO45ZbbmHUqFFNp8xcdNFFLFq0iPnz51NTU8OqVavYsGEDZ5xxBrfddht77703H/rQh7jwwgv53Oc+B8CQIUO4//77efnllznppJO49dZb6dOnD9/97nc5//zz+epXv9o5vztJhWGuM9dJlcBcZ66T8tjh7UYOOOCApqQIcMEFF7Dffvtx0EEHsWTJEp566qmttpkwYQIzZswAYPbs2SxatGirMYcccghnnHEGF198cdNpK7feeisf//jHqanJvvMYPHgwTzzxBBMmTGDvvfcG4PTTT+euu+5q2s/73vc+AO655x4effRRDjnkEGbMmMFll13G4sWLO+V3IKn8meskVQJzndQ92OGF7X5j15X69OnT9PiOO+7g1ltv5e6776Z3794cccQRbNiwYattevbs2fS4urq6zVNffvKTn3Dvvfdy4403Mnv2bObNm7dL8aWUOProo7nqqqt2aj+SisRct0PxmeukEmWu26H4zHUqd3Z4i6Rfv36sXr16m6+/9tprDBo0iN69e/P4449zzz337PR7Pf300xx44IGcd955DBs2jCVLlnD00Ufz05/+tGkihVWrVrHPPvuwaNEiFi5cCMAVV1zB4YcfvtX+DjroIP761782jVu7di1PPvnkTscnqXyZ6yRVAnOd1H1Z8BbJkCFDOOSQQ5g6dSrnnHPOVq8fe+yxNDQ0MGnSJM4991wOOuignX6vc845h2nTpjF16lTe9KY3sd9++/HRj36UsWPHMn36dPbbbz+uvPJK6urquPTSSzn55JOZNm0aVVVVfOITn9hqf8OGDWPOnDmceuqpTJ8+nYMPPpjHH398p+OTVL7MdZIqgblO6r4ipVTsGAquvr4+zZ07t8W6xx57jEmTJm09+PXluQcBkfvZJPKexo6ti2g1ZhfWtYhrF9dtM87Ssc1jKW1HRMxLKdUXO47OtEO5TiXHY6mdVW75zlxX3jyW2lnbynVew9vampeA8v8SoH3bK+hpowhv9cVA049OKMK3uy5g3Uq4/tNZDFGVLVXVzY+bloABY2DaydB7cEd+CZIkSZJKmAVva7vPaH7c1P1OeT/y1jXVxTu7Lq+wbtFpTzu5bhtxdmhdW3G1FXsb79fWuO3G1NZ7bVmXtv6M2/xd5X42bIRnboe0eRtLyn5uboSG9fCH/weTT4D6M2HswSXZ1ZYkSZLUPgve7WnzVGJ1O6uq4POPdmzsCw/DvDmw4Jfw0DUwbF+YfQbsdwr0GlTIKCVJkiR1sYqetKoSrl8udzt8DEdMhbd/D/75cTj+h9CjL/z+XPivfeHXH4fn7mnVYZZKX5v/Tt5YB2+shU3rYdMGaNgADW9A4yZobMjOiNi82X8P3YT/X0nt899J6fMYqhAK2uGNiGOB/wGqgf9NKX2n1evjgEuAYcAq4LSU0tKIOBL477yh+wKnpJSuj4g5wOHAa7nXzkgpzd/R2Orq6li5ciVDhgwhPKW1JKWUWLlyJXV1dTu+cY8+MOuD2fL8glzX9xpYcDUMn5x1fae/D3oN7OSoVY5KMtetegY2b9qBPUXurJd2fnbWmBY/q3Zh2y2PS9cu5TqpE5VkrlPJMNepUAo2S3NEVANPAkcDS4H7gFNTSo/mjbkW+F1K6bKIeAtwZkrpg632MxhYCIxOKa3LJcbfpZSu62gsbc3mt2nTJpYuXdrmTb9VOurq6hg9ejS1tbW7vrONa+DhX2XF7/L7oaYXTD0pK35H71/yfzSrpc6atbRkc13Dhlz3Nv86+W09puW189t8vL1t23lccPmF7/Ye535Cc9Hc5uOObtvePtvYTxvbdGquU8XpjHxXsrlOJcVcp11RjFmaDwAWppSeyQVwNXACkH+x5WTg87nHtwPXt7Gf9wA3p5TWdWZwtbW1TJgwoTN3qVLXsy/MPj1bls/PCt+HroX5v4DhU7JJrqa/F+oGFDtSdS/mul21uTF3KvUbsLkh73Hu9Op2H+eWrR6/kTs9u73Hm3Lvu5OPC160B1T3gOpaqKrp2ON+I2HGqTDhCKiq6KuX1HnMdZJKUiEL3lHAkrznS4EDW415EDiJ7PSYE4F+ETEkpbQyb8wpwPmttvtWRHwVuA04N6W0sVMjl3afAbt/H475Bjx0Hcy7FG76Avzxq7mu74dh1Cy7vgJz3a6rqs6W2hI9jW1z4zaK4rwCfKtifluPO1rkt1PwL/xjNjHfoPEw63SYeRr0HV7s35RKm7lOUkkq9izNXwB+GBFnAHcBy4DGLS9GxEhgGnBL3jZfAl4AegAXAV8Ezmu944g4CzgLYOzYsYWJXuWvZ7+ss1t/Jiy7P9f1vQ4e+DmMmJad7jztvVDXv9iRqnsz15Wzqmqo6gW1vYodSbNNG+Cx/8ty1m3/Brf/O+z79iyXjT/Mrq8KxVwnqdsp5P94y4Axec9H59Y1SSktTymdlFKaCXw5t+7VvCHvBX6TUtqUt83zKbMRuJTsFJutpJQuSinVp5Tqhw0b1ikfSBVu1Cw4/oJshue3576cvvGfsxmeb/hMVhCrEpnr1P3U1sH0k+HMG+HT98EBZ8Gzd8LlJ8APZ8Nfvg9rVhQ7SpUWc52kklTIgvc+YGJETIiIHmSnsNyQPyAihkbElhi+RDazX75TgatabTMy9zOAdwEPd37o0nbU9Yf9PwIf/zN89E/ZKc4PXQcXHwk/PQzmXgIbVxc7SnUdc526t2F7w7H/Dp9/HE68CPqOgFu/BudPgmvPhGfv8vZT6ghznaSSVLCCN6XUAJxNdtrKY8A1KaVHIuK8iDg+N+wI4ImIeBLYDfjWlu0jYjzZN4l3ttr1LyLiIeAhYCjwzUJ9Bmm7ImD0bDjhh1nX97jvZdfy/e6fsq7v//1jNvmVypq5TiWjtg72ex98+Gb41L2w/0fh6dvgsnfCD2bDXy+AtSvb348qkrlOUqkq2G2JupO2pq+XCiIlWDo3m+Tq4V9Dw3rYfWZ2re/U92QzQatb6KzbEnUn5jrtsE3r4ZHrs2t9l9yTzfQ86fjsWt9xhzgxX5kot3xnrpPUlm3lOmetkDpTBIzZH97146zr+7b/hIaNWbf3v/bNur/PLyh2lJKUqe2V3b7oI7fAJ++G2WfCU3+EOW+HH+4Pf/shrFtV7CglSdppFrxSofQaCAeeBZ/8G3z4DzDpHTD/SvjpoXDxW+D+y+GNtcWOUpIyu02G4/4j+7LuhB9Dr0Hwhy9nX9b96mOw+G9e6ytJKjkWvFKhRcDYA+HEn2R/SB773azQveEz2R+SN/4zvOAcHZK6iR69YeYH4KN/hE/8FWZ9CJ78PVz6NvjRgXD3j+36SpJKhgWv1JV6DYKDPgGfugfO/D3s8za4/wr4ySHwv2/N7u/7xrpiRylJmRFT4e3fy76sO/6H2b3Jb/lSNsPzrz8Oz91j11eS1K1Z8ErFEAHjDoaTLsr+kPyHb8OG1+C3n866vjedAy8+WuwoJSnTow/M+iB87LbslmwzPgCP3wiX/AP8+GC45yew/pViRylJ0lYseKVi6z0YDv4UfPrvcMZNsPcx2YypFx4MPzsG5l+VzaQqSd3ByOnwjvOzL+veeUE28dXvv5h9WfebT8KSv9v1lSR1Gxa8UncRAeMPgXf/L3z+cTjmW7BuJVz/CfivfeDmL8JLjxc7SknK9OwLs0+Hs26Hs+6E/U6Fx26Anx0NFx4C914E618tdpSSpApnwSt1R32GwJvOhrPnwum/g73eCvf9DH58IFxyLDz4S9i0odhRSlJm9xnwzu9nXd93fB+qa+Dmc7Ku7/Wfzu5PbtdXklQENcUOQNJ2RMCEQ7Nl7cvZbY3mzYHfnJWdQrjfqdl9M4ftXexIJSmb1Kr+zGxZ/gDMvRQeug7m/xx2mwqzz4Dp74W6AcWOVJJUIezwSqWiz1A45LNZ1/dDN8AeR8DfL4Yf7Q+XHgcLroWGjcWOUpIyu8+E4y/Iur5vPz/7Au+mL2Rd39+eDcvm2fWVJBWcHV6p1FRVwR6HZ8uaFVnnZN4c+PVH4ebBMOP9WRdl6MRiRypJUNcf9v8I1H8Ylt+fdX0f/hU8cAWMmJadpTLt5GycJEmdzA6vVMr6DoM3/xN85gH44PXZqc/3/gR+WA9z3pGdSmjXV1J3EAGjZsMJP8y6vsd9L+vw3vj5rOt7w2dh2f3FjlKSVGbs8ErloKoK9jwyW1a/mOv6Xga/+gj0HpLdM3P2GTBkz2JHKknZNbwHfAz2/2h2avPcS2HBNXD/ZTByv1zX9z3ZNcGSJO0CO7xSuem3Gxz6z/DZ+XDar2Hcm+DuH8EPZsFl74SHfw0NbxQ7SknKur6j6+FdP8q6vm/7T2jcBL/7XNb1/b/PwfL5RQ5SklTK7PBK5aqqCvY6KltWv5BdLzfvcrjuTOgzLNf1PR0G71HsSCUJeg2EA8/KOr9L78u6vg9eBfMuzSbAmn0mTH13dv9fSZI6yA6vVAn6jYDDzoF/nA8f+BWMORD+9gO4YCZcfgI8cn3WVZGkYouAMQfAiRfmur7/kd13/P8+m3V9f/d5eH5BsaOUJJUIO7xSJamqholvzZbXl8MDuWt9rz0d+gyHmadlXd9B44sdqSRBr0Fw4MfhgLNgyb1Z1/eBn8Pcn2UTYM0+E6aeBD36FDtSSVI3ZYdXqlT9d4fD/wU+twDef212Hd1fvw//MwOuOBEevcGur6TuIQLGHgQn/TTr+h77Hdi4Bm44O+v63vgFeOHhYkcpSeqG7PBKla6qGvY+JlteW5Zd63v/5XDNB6HviKzrO+tDMGhcsSOVJOg9GA76JBz4CXju7qzre//lcN/FMHr/rOs75UTo0bvYkUqSugE7vJKaDRgFR5wL/7gATr06uz3IX86H/9kPfv5ueOx30NhQ7CglKev6jnsTvPvirOv7D/8O61+F334q6/re9C/w4qPFjlKSVGR2eCVtrboG9nlbtry6pLnr+8sPQL+RMPODWdd34JhiRypJWdf34E/DQZ+CxX/Nur7zLoW//zSbpG/2mTDlXVDbq9iRSpK6mB1eSds3cAwc+a/wuYfhlCtht6lw13/C96fBL06Gx2+y6yupe4iA8W+G9/wMPv84HPNNWLcSrv8E/Nc+cPMX4aXHix2lJKkL2eGV1DHVNbDv27Pl1eeyju/9V8DVp0L/Uc1d3wGjih2pJEGfIfCmz8DBZ8OiP2dd3/t+Bvf+BMYenHV9J58AtXXFjlSSVEB2eCXtuIFj4S1fgX96GN73cxi2L9z5Xfj+VLjyFHjyFtjcWOwoJSnr+k44DE6+FD7/GBx9Hqx5EX5zFpy/L/z+S7DiyWJHKUkqEDu8knZedS1Meme2vLKouev75M3Qf3TW8Z31wewWSJJUbH2HwSH/CAd/BhbdlXV9/34R3PNjGHdIrut7PNT0LHakkqROYodXUucYNB6O+ip8/lF47+UwdCLc8e/w31PhqvfDU3+06yupe6iqgj2OgPdelnV93/p1eH0Z/Pqj2QzPt3wZXn6q2FFKkjqBHV5Jnau6NrsubvIJsOpZuP8yeODn8MSNMGBs1vWdeRr0H1nsSCUJ+g6HN/8TvOkf4dk7sq7vvT+Bu38I4w+F2WdkZ7HY9ZWkkmTBK6lwBk/IOidH/GtW8M69FG7/Jtzx7eyWR/Vnwh5vybotklRMVVWw51uyZfWLMP/nMG8O/Ooj0HsIzPhAVvwO2bPYkUqSdoAFr6TCq+kBU07MlpVP57q+v4DHf5dNgDXr9GyW5367FTtSScpy0aH/DIf8Ezzzp+zLurt/BH+7IJsAa/aZsO87stwmSerWLHglda0he2azpB755azgnXsp/Okbua7vcVnXd8IRdn0lFV9VFez11mx5/flc1/dyuO5M6DMs1/U9HQbvUexIJUnbYMErqThqesLUd2fLywvh/jlZ1/exG7IJsGafkf0x2Xd4kQOVJLJ5Bw47B978eXg61/X92w/gr9/PJsCafWZ2n/Lq2mJHKknKYwtFUvEN3QuO+Sb88+Pw7p9ltzS69etw/mS45nR45g7YvLnYUUoSVFXDxKPh1Cuze5Ef+eXsS7trT89y1q3/lk3YJ0nqFgpa8EbEsRHxREQsjIhz23h9XETcFhELIuKOiBid91pjRMzPLTfkrZ8QEffm9vnLiPACGqlc1PSEae+BM2+ET98HB5wFz94Jl58AP5wNf/0fWPtysaPcirlOqlD9d4fD/wU+twDefw2Mmp11fC+YAVecCI/eAI2bih1lpzHXSSpFBSt4I6Ia+BHwNmAycGpETG417HvA5Sml6cB5wLfzXlufUpqRW47PW/9d4L9TSnsBrwAfKdRnkFREw/aGY/8dPv84nHQx9B0Bf/xqdo/Ma8+EZ++ClIodpblOUtb13fsf4P1Xw+cehiO+BCuegGs+CP89BW77BryyuNhR7hJznaRSVcgO7wHAwpTSMymlN4CrgRNajZkM/Cn3+PY2Xm8hIgJ4C3BdbtVlwLs6K2BJ3VBtHUx/L3z4ZvjUvbD/R7Pr5y57J/ywPruGbu3KYkZorpPUbMAoOOJc+McFcOrVMHIG/OV8+J/94Ofvhsd+B40NxY5yZ5jrJJWkQk5aNQpYkvd8KXBgqzEPAicB/wOcCPSLiCEppZVAXUTMBRqA76SUrgeGAK+mlBry9jmqrTePiLOAswDGjh3bKR9IUpEN3xfe9h1469fg0d9mk8b84Stw23kw6fisqzJ0r66OylwnaWvVNdn9xvd5G7y6BB64Au6/HH75geyMlVkfhFkfym7NVhpKM9fd8Fl4+alsMrHqHtlSk/uZv26rxz3bH1PTs5199Mj2U1UNER2PWVKnKvYszV8AfhgRZwB3AcuAxtxr41JKyyJiD+BPEfEQ8FpHd5xSugi4CKC+vr745z1K6jy1vWC/U7LlxUdh3hxYcHXWVemezHVSJRs4Bo78VzjsX+CpW7Iv6+76XrZMPDqb4XniMVmRXNq6X66rqcsKzoYNsPH17JrqxjdyS6vHDRshNba/zx0W2y+KmwrwbRXNO1KAt96ujQJ8W/uwMFeZKmRmXQaMyXs+OreuSUppOdk3gUREX+DdKaVXc68ty/18JiLuAGYCvwIGRkRN7tvArfYpqcLsNhmO+w845hvZf/Zdz1wnqWOqa7JbF+37dnj1uazje/8VcPWp0G/35q7vgNHt76vrlWauO+4/dmz85sa8Qrit4njj9ovmLY8b2nm9cVNuX2+0fL9N62HDa83rGja2HU8hC/MWnettFdgd6JJvVYBvq5PeTpe8dSe9qroAn13lrJAF733AxIiYQJa8TgHenz8gIoYCq1JKm4EvAZfk1g8C1qWUNubGHAL8R0opRcTtwHvIrh05HfhtAT+DpFJRnGIXzHWSdsbAsfCWr8DhX4Qnf591fe/8D7jrP7Nu7+wzs+5v9/njvjJyXVV1ttTWFTWMdm1VmLcunrfRwd7mmB0p6DfBG2uh8ZX295UKcEvBqGoufusGwNiDs3th73F4d/2ySEVWsII3pdQQEWcDtwDVwCUppUci4jxgbkrpBuAI4NsRkchOffl0bvNJwE8jYjPZxFrfSSk9mnvti8DVEfFN4AHgZ4X6DJLUHnOdpF1SXQuT3pktryxq7vo++XvoPyrr+M78YDYZVhGZ67qZkirMt1M8N3SwY76t11e/AM/cDg9dk73fkL1gwuFZATzhUOg1qKgfX91DpG5wW49Cq6+vT3Pnzi12GJK6kYiYl1KqL3YcnclcJ5WJxk3wxE1Z1/eZ27OO1t7HZl3fvY7a4a5vueU7c51aSAleehSeuSNbFv0VNq0FAnafkSt+D4exB2VzgKhsbSvXlfzsCJIkSWWluhYmn5Atq56F+y+DB36eFcEDxsA7vg8T31rsKKXuIQJ2m5ItB386u3562Tx49s6sAP7bD+Av/51dLzz2wFwBfERWDHefSwZUQBa8kiRJ3dXgCfDWr8MR/wpP3Jh1ffuPLHZUUvdV0wPGHZwtR5wLG9fA4r81F8C3nQecl13/O/7Q5g7w0InOUl2mLHglSZK6u5oeMOXEbJHUcT37wt7HZAvAmhVZ8bulAH78d9n6frtnE19tKYD9YqlsWPBKkiRJqgx9h8G092QLZJcNPHNHVgA/eQs8eFW2fug+zQXw+DdnHWGVJAteSZIkSZVp8IRsqT8TNm+GFx+CZ3Ld3wd+Dn+/KJs4bvdZzbc/GnNgMW+HqB1kwStJkiRJVVUwcr9sOeSz2W2Tlt6XFcDP3plNfvXn70FNr2zW5y0F8IjpToDVjVnwSpIkSVJrNT2z05nHvxn4Mmx4HRb/tbkDfOvXsnG9BjVPgLXHETB4DyfA6kYseCVJkiSpPXX9YZ+3ZQvA6hfg2buaC+DHbsjWDxiTTXy1xxEw4TDot1uxIhYWvJIkSZK04/qNgOnvzZaUYNUz8MztWQH8+O9g/s+zccMnN8/+PP4Q6NmvqGFXGgteSZIkSdoVETBkz2zZ/6OwuRFeWJB1fp+5E+ZeAvf8GKpqYNTs5gJ49P7ZbcdUMBa8kiRJktSZqqph95nZ8uZ/gk0bYOnfcwXwHXDXf8Kd34Xa3jDuTc0F8G5Ts8mz1GkseCVJkiSpkGrrsut5JxwGR30V1r8Ki/6Szf78zB3wh69k43oPycZsmQBr0PiihVwuLHglSZIkqSv1GgiT3pEtAK8vb7790TN3wCO/ydYPHNd8+6MJh0OfoUUKuHRZ8EqSJElSMfXfHWacmi0pwctPZYXvs3fCI9fD/Zdl43ablhW/exwBYw+Gnn2LGHRpsOCVJEmSpO4iAobtnS0HngWNDfD8g9kM0M/eCX+/CO7+IVTVZpNebekAj5oN1bXFjr7bseCVJEmSpO6qugZGz86Ww74Ab6yDJfc03//3jm/DHf8OPfrCuEOaC+Dhk7PiucJZ8EqSJElSqejRG/Z8S7YArFsFi/7cXAA/dUu2vs/wvAmwDoeBY4sVcVFZ8EqSJElSqeo9GCafkC0Ary7JTX6VK4Afvi5bP3iP5tsfTTgs264CWPBKkiRJUrkYOAZmnpYtKcGKx3P3/70TFlwLcy8BAkZOby6Axx6cdY7LkAWvJEmSJJWjCBg+KVsO+iQ0boLlDzQXwHf/GP76P1DdA8YcmLv90RGw+8zs2uEyUB6fQpIkSZK0fdW1MOaAbDn8X+CNtbD4bnj2jqwI/tM3gW9Cz/4w/s3NHeBh+5TsBFgWvJIkSZJUiXr0gYlvzRaAtS/Ds3flrgG+A564KVvfd0Tz5FcTDocBo4oV8Q6z4JUkSZIkQZ+hMPWkbAF4ZVF26vOzd8LCW2HB1dn6IRObC+Dxb4Zeg4oVcbsseCVJFSulROPmRMPm1j83Zz8bt7F+y/PGbazfnGjcvLmN7XPrNycaG7exfsvzxm2s78D+e9VWs/du/Zg0sn9u6cfA3j2K/euWJJWaQeNh9niYfTps3gwvPZp1fp+9E+ZfCfddDFEFI2c0F8BjDoLauqKGnc+CV5IqWLkWfFvH1/Z+GjenYh8CqgJqqqqorgpqqoLq6tzPqmi5fsu66qC6qqppXY+aKno1jcnWr964iT89/hLXzlva9D4jB9Q1Fb/7jsgK4QlD+1BdVZrXZEmSulhVFYyYmi1vOhsa3oBl83ITYN0Bf7sA/nI+1NTlJsA6IiuAR86AquqihW3Bq5KweXNiU94f1A2NLf8g3/LH7Ka8P24bGpv/uN3UmI3tVVvNmMG92X1gL//IU0U77D9uZ9mr67tFwddUyLX42VzQZQVeG+u3U/C1LBrb2K562/vLXm9rfceK0prt7ruqxbbVEVQVMBe9tHoDjz2/mseff53Hnn+dx55fzV1PrqAhd9x71lSxz4h+TBrRn31H5jrCI/ozoHdtwWKSJJWJmh4w7uBsOfJLsHE1LP5b8/1/b/s3uA2oGwDjD80VwEfCkD27dAIsC94SlVJic4KGXIejuZuxdZHXukDclNeRabFtXrdkU4tisrkbsymvS9Kwjf20fN+WnZa2Ymj5PLeusflxw+ZE6uS/yWurgzGDejN2SG/GD+nD2MG9GTekN+OG9GHM4F70rCnet1BSVzjlgDGs29jYsQKwejvF21YFadsFYJvrcz+jRGd9LAXD+9UxvF8dh+89rGndxoZGFr60prkQfuF1/vjYi/xy7pKmMaMG9mLfEc2nRO87sh/jh9gNliRtR89+sPc/ZAvAmpeyCbC23ALp8d9l6/uPap79eY/Dod+IgoZlwdvK7U+81KLY2qroayrONucVks2n1rVX9LXetjGvqNtWsbmtorVY8v9YrcnrgtRWV+Wtb/4DuSavm9GntqZ5TF6noyavK1O75Y/uptdaPt/6vfJjaPm+W/7I3rLtmo0NPLdyHYtWruO5VWtZvHIdcxe9wpqNDU2fLwJ2H9CrRRE8bkjvpuf96ux8qPR96oi9ih2CiqRnTTVTdh/AlN0HNK1LKbFi9UYezXWBH38h6wjf8eSKprMAetVWs/eIfkzKK4T3GdGPAb3MiZKkNvQdDtPeky0pwSvPNhe/T9wE83+RjRu2b3MBPP6QrCPciSx4W/nInPvYmTP8tlX05XdAthRutXndkJqqKno2FYFtFH3b27aDRV+7BWPuteqqoLZFEdq6KC2Pbsyb9mz5PKXEqrVvsHjVOhavzIrgrChey62PvcjLa95oMX5Inx5tdobHDenNkD49Sv73I6nyRATD+9cxvH8dR+wzvGn9hk1busHNhfAtj7zA1fe17AZvuTZ4SyE8bnDvgp6qLUkqMREweI9sqf9wNgHWiw81F8DzLoN7fwJRDaNmZQXwlBNhtym7/NYWvK386pNv2qpI3VL01Va17Bh6Sl55iAiG9O3JkL49mTV26ynV12xsYPHKtVt1hv/+7Cqun7+sxenWfXvWbFUEjxvcm3FD+zCif52nA0oqKXW11UwdNYCpo1p2g198fWNWBL+QFcKPPf86f3r8xaYvjHvVVmfXBucVwvuO6OcZMpKkTFUVjNwvWw75R2jYCEv+3nz/3z+fD313s+AthJltFDyqbH171mx1+t8WGxsaWfrK+qbOcLas5YkXV3PrYy+2OPW8R3UVowf3atEZHj+kD2OH9Gb0IK8bllQaIoIRA+oYMaCOI/dt2Q1+6sU1eYXw69z00PNc9ffnmsaMHtSr+VZJuYJ4rN1gSVJNT5hwaLa85Suw4TWgc/5vsOCVdkHPmmr2HNaXPYf13eq1xs2J519b39QZXryquUt87zMrWftGY9PYLdcNb9UZzj3u09N/qpK6t7raaqaNHsC00S27wS+8vqHplOjHcrNF3/ZYcze4d4/8bnBWCO87sj99zXuSVLk68Tregv5vEhHHAv8DVAP/m1L6TqvXxwGXAMOAVcBpKaWlETEDuBDoDzQC30op/TK3zRzgcOC13G7OSCnNL+TnkHZGdVUwelBvRg/qzZtazQ+UUmLl2je26gwvXrWOPzzyAivXtrxueGjfHlnxOzhvZulcUTzY64aLzlwntS0iGDmgFyMH9OIt++7WtH7DpkaefHF1i0L4dw8u58p7m7vBYwf3bjFT9KSR/RgzyG5wMZnrJJWighW8EVEN/Ag4GlgK3BcRN6SUHs0b9j3g8pTSZRHxFuDbwAeBdcCHUkpPRcTuwLyIuCWl9Gpuu3NSStcVKnap0CKCoX17MrRvT2aPG7zV66s3bGouhJs6w2u555mV/KbVdcP9etZkxe+W7nBeZ3hE/zr/OCwwc5204+pqq5k+eiDTRw9sWpdSYvlrG1rcM3jLLZO25Lw+ParZN3c9cP5M0XaDC89cJ6lUFfJ/iAOAhSmlZwAi4mrgBCA/MU4GPp97fDtwPUBK6cktA1JKyyPiJbJvC18tYLxSt9GvrnariWK22LCpkaWvrNuqM/z486v546OtrhuuqcquF26jMzx6UG961FR15ccqV+Y6qRNEBKMG9mLUwF4cNam5G7z+jUaeeHF1i0L4hgeX84u8bvC4Ib2ZNCK7X/Ckkf2ZPLI/owf18uyXzmWuk1SSClnwjgKW5D1fChzYasyDwElkp8ecCPSLiCEppZVbBkTEAUAP4Om87b4VEV8FbgPOTSltbP3mEXEWcBbA2LFjd/3TSN1EXW01ew3vx17D+231WuPmxPJX1/PcqqwjvKUzvHjlOu5+ZiXr8q4brgrYfWCvNjvD44b0pncPOyYdZK6TCqhXj2pmjBnIjDEDm9allFj26vrsVkl5s0Xf8ugLTd3gvj1rmjrB++bNFG1u22nmOkklqdhZ/wvADyPiDOAuYBnZtR0ARMRI4Arg9JTS5tzqLwEvkCXLi4AvAue13nFK6aLc69TX1+/EnXWl0lNdFYwZ3Jsxg3tzyF5DW7yWUmLFmo0816ozvHjlOm5+6HleWbepxfhh/Xq26AyPG9KbsYOzxwN719o52THmOqkTRTTPkXD05OZu8Lo3GnjihdVN9wx+7PnXuf6BZay+pyG3HYwb3LvpdOgtBbHd4E5jrpPU7RSy4F0GjMl7Pjq3rklKaTnZN4FERF/g3Vuu54iI/sCNwJdTSvfkbfN87uHGiLiULLlKakdEMLxfHcP71VE/fuvrhl/fsKlFR3jL47ufXsmv72/xT5d+dTWtOsPN3eHd+lXcdcPmOqmb6N2jhpljB7W4xWBKiaWvrN9qpuibH36haUy/njVNXeAthfA+doNbM9dJKkmFzOT3ARMjYgJZQjwFeH/+gIgYCqzKfcv3JbKZ/YiIHsBvyCY+uK7VNiNTSs9H9lXsu4CHC/gZpIrRv53rhpfkusGLVq7ludzjR5a9xi0Pv0DD5uYv23tuuW44rwje0hkeNagXtdVld92wuU7qxiKaz3w5ZsqIpvVrNzbw+AvNneDHnl/Nr+YtbbplXARMGNInK4RH9G86NXrUwIrtBpvrJJWkghW8KaWGiDgbuIVs+vpLUkqPRMR5wNyU0g3AEcC3IyKRnfry6dzm7wUOA4bkTouB5mnqfxERw8juRDwf+EShPoOkTF1tNRN368fE3ba+brihcTPPv7ahuTO8ah2LXs6K4r8uXMn6Tc3XDVdXBbsPrMsmz2qjKC7Fboq5TipNfXrWMHvcIGaPa+4Gb96cdYMfff71pkL44WWvc9NDzd3g/nU17Ju7X/CWjvDeu/WjV4/qYnyMLmOuk1SqIqXyvwyivr4+zZ07t9hhSBUnpcSK1RtZnFcE518//Gqr64aH9+vZ4lTp/OuHB/bu0amxRcS8lFJ9p+60yMx1UmGs2djAEy+8zqPPN88W/fgLq5smAqwKGD+0T9MM0VuuDR45oK5bdIPLLd+Z6yS1ZVu5rvTaKZJKRkQwvH8dw/vXsX8b1w2/tm4Ti1e17AwvXrWOvzz1Mte9vqHF2P51NYwf2qoznJtZeni/npV23bCkLtS3Zw2zxw1ucd/0zZsTS15Zx2PPNxfCC5a+yo0Lnm8aM6BXbVPxOzl3SvTeu/Wjrra8u8GS1J1Y8EoqmgG9a5neeyDTRw/c6rX1bzSy5JWWneFFK9fy0LLXuPnhF2jMu264rja7bvh7J+/X5r4kqbNVVUXukow+HDt1ZNP61Rs25WaKzhXCL7zONXOXtOgGT8h1g7MlK4hH9O8e3WBJKjcWvJK6pV49qtl7t6wb0tqmxs0sf3V9dnr0qnUsznWGB/bq3NOeJWlH9aurpX784Baz4W/enFi8al3T6dCPPr+a+Ute5Xd53eCBvWuZNKL5nsGTR/Znr+F97QZL0i6y4JVUcmqrq5o6K5LU3VVVBROG9mHC0D68bVpzN/j1DZt4PO+ewY8+v5qr/v4cGzZlt6itrgr2yHWD8wvh4f162g2WpA5qt+CNiL2BC4HdUkpTI2I6cHxK6ZsFj06Suoi5TlJX619XywETBnPAhOZucOPmxOKVa3ksrxCet/gVbnhwedOY89+7HyfNGr1T72muk1RpOtLhvRg4B/gpQEppQURcCZgYJZUTc52koquuCvYY1pc9hvXl7dObu8Gvrd/UdEp0W5MA7oCKynXLX11P4+ZEdVVQUxXUVFflPQ5qqqqoCuyYS2WsIwVv75TS31slgoYCxSNJxWKuk9RtDehVy4F7DOHAPYbs6q4qKtd9+sr7eeC5V9sdV1sduUI4K4jzn9c0PW7veRU1VUF1dVBbFVTnnmeFde559ZbtWj7P3rO5GG/9vKa6qmn9lkK9dZxtxd3WvizuVWk6UvC+HBF7AgkgIt4DPL/9TSSp5JjrJFWCisp1nz1qIivXvEFD42YaNicaNycaNqeWz/Meb2pMNG5u+3lDY8qtb37euDmxoaExt5/cNps3Nz1v2PK41fNNjan94Aukuiq2U1g3F89bnjcV7nnPa1sV320V460L9drqbXwJ0LTf7RfqbX0J0Ku2mt36e027tq8jBe+ngYuAfSNiGfAs8IGCRiVJXc9cJ6kSVFSuO3Kf4cUOoU0pJTYnWhTAWWG8ualwbl2Yb2psLp5bP29RXOcV5pvy9tX6ecsvAVq9b97zTVuK/MbEhk2badjc2PS89fZtx7aZzQWs7/v0qGav4X3Za3g/Ju7Wl4nD+7LX8L6MHtSb6ioLYbVT8EZENfCplNJbI6IPUJVSWt01oUlS1zDXSaoE5rruIyKoDqiuqozbTm3enGhM7Xe92yqet9edX71hE0+vWMtTL63mz0+t4Ff3L216z541Vew5rG9eEZwVxOMG96amuqqIvw11te0WvCmlxoh4c+7x2q4JSZK6lrlOUiUw16lYqqqCKoLsttKFK/JfW7eJhStWs/ClNTz14hqeemkNcxe9wm/nN89yXlud3SZs4vB+7DU8K4j3Gt6XCUP70LOmMr6AqDQdOaX5gYi4AbgWaEqOKaVfFywqSep65jpJlcBcp7I1oHcts8cNZva4ljOZr9nYwNMvrckK4ZfWsPCl1Ty8/DVuevh5Uu506+qqYNzg3k1F8JaCeM9hfenVw0K4lHWk4K0DVgJvyVuXABOjpHJirpNUCcx1qjh9e9aw35iB7DdmYIv1GzY18kzulOjmrvBqbnv8JRpzFx5HwOhBvZg4vF/T9cFbln51tUX4NNpR7Ra8KaUzuyIQSSomc52kSmCuk5rV1VYzeff+TN69f4v1bzRsZtHKtS2K4IUvreEvT73MG42bm8aNHFCXdYRz1wdnj/sysHePrv4o2o52C96IGA38ADgkt+rPwD+mlJZueytJKi3mOkmVwFwnta9HTRV779aPvXfrB9Oa1zc0bmbJK+t56sXVuVOjs2L4yr8vZsOm5kJ4aN+eTBzePGHWnrmieGjfHt5CqQg6ckrzpcCVwMm556fl1h1dqKAkqQjMdZIqgblO2kk11VVMGNqHCUP7cMyU5vWbNyeWvbq+qQDeMmHWr+9fxpqNDU3jBvaubZ4xOnda9MTd+jKif52FcAF1pOAdllK6NO/5nIj4XIHikaRiMddJqgTmOqmTVVUFYwb3Zszg3hy5b/O9n1NKvPj6xhZF8MKXVnPzw89z1bpNTeP69qxpOh26+dTofowa2Isq7yW8yzpS8K6MiNOAq3LPTyWb7ECSyom5TlIlMNdJXSQiGDGgjhED6jh04rCm9SklVq59g6dezArgp3LXCt/x5Aqundd8dUFdbVVT8btloqyJw/sy1nsJ75COFLwfJrvW47/JZvH7G+CEB5LKjblOUiUw10lFFhEM7duToX17cvCeQ1q89uq6N5pun7Rlwqx7n1nJbx5Y1jSmR3UVewzr06IYnrhbX8YP6UOPGgvh1joyS/Ni4PguiEWSisZcJ6kSmOuk7m1g7x7Ujx9M/fiW9xJevWETT69Yy1Mvrm4qiBcsfY0bH2p5L+HxQ3q3mDV6y72E62or917CHZml+TKy2ftezT0fBPxXSunDBY5NkrqMuU5SJTDXSaWpX10tM8YMZEarewmvf6ORp1esaTFh1pMvreaPj73Y4l7CYwf3bpowa8up0XsN70ufnh054be0deQTTt+SFAFSSq9ExMzChSRJRWGuk1QJzHVSGenVo5qpowYwddSAFus3NjSy6OV1TUXwwhVrWPjiGu58cgWbGlPTuFEDe7UogCfu1pe9hvVjQO/arv4oBdORgrcqIgallF4BiIjBHdxOkkqJuU5SJTDXSRWgZ001+4zoxz4j+rVY39C4mcWr1jVNmLXl9Oh7nlnJxobmewkP79czdx/hfrn7CGfLkL49u/qj7LKOJLj/Au6OiGuBAN4DfKugUUlS1zPXSaoE5jqpgtVUV7HnsOy6XhjRtL5xc2LZK+uzjvBLa5oK4WvnLmHtG41N4wb36dGyI5y7Xnh4v57d9l7CHZm06vKImAu8JbfqpJTSo4UNS5K6lrlOUiUw10lqS3VVMHZIb8YO6c1Rk3ZrWp9S4vnXNjQVwQtzp0j/34PLeX1DQ9O4fnU1uS5w7hZKu2VF8e4Din8v4Y5MWrUn8HRK6dGIOAJ4a0Qsz7/+Q5JKnblOUiUw10naERHB7gN7sfvAXhy+d8t7Ca9Ys5GFueuDt9xC6bbHX+SXc5c0jevdo7pptuimjvDwvowZ3JvqLiqEO3JK86+A+ojYC/gpcANwJXBcIQOTpC5mrpNUCcx1knZZRDC8Xx3D+9Xxpr2Gtnht1do3mmaNXpjrDP9t4Up+fX/evYRrslOrm0+NzibMGjekD7XVnXsv4Y4UvJtTSg0RcRLww5TSDyLigU6NQpKKz1wnqRKY6yQV1OA+PThgwmAOmNDyXsKvb9jUVAAvfGkNT724mvufe4UbHlzeNKamKpgwtA97De/L+/YfwxH7DN/leDpS8G6KiFOBDwHvzK0rn3mqJSljrpNUCcx1koqif10ts8YOYtbYQS3Wr3ujgadfWsvCFatzp0av4fEXVrNi9cZOed+OFLxnAp8AvpVSejYiJgBXdMq7S1L3Ya6TVAnMdZK6ld49apg2egDTRg9of/BO6MgszY8CnwWIiFkppfuB7xYkGkkqEnOdpEpgrpNUaXb0iuD/3ZHBEXFsRDwREQsj4tw2Xh8XEbdFxIKIuCMiRue9dnpEPJVbTs9bPzsiHsrt84Lorjd8klTKzHWSKoG5TlLZ29GCt8NJKCKqgR8BbwMmA6dGxORWw74HXJ5Smg6cB3w7t+1g4GvAgcABwNciYsvJ3hcCHwMm5pZjd/AzSFJ7zHWSKoG5TlLZ29GC9992YOwBwMKU0jMppTeAq4ETWo2ZDPwp9/j2vNf/AfhjSmlVSukV4I/AsRExEuifUronpZSAy4F37eBnkKT2mOskVQJznaSyt82CNyLOzns8BSCldP0O7HsUsCTv+dLcunwPAiflHp8I9IuIIdvZdlTu8fb2KUkdZq6TVAnMdZIq1fY6vB/Oe1yo2fu+AByeu//b4cAyoLEzdhwRZ0XE3IiYu2LFis7YpaTyZK6TVAnMdZIqUkdPad6ZCQSWAWPyno/OrWuSUlqeUjoppTQT+HJu3avb2XZZ7vE295m374tSSvUppfphw4btRPiSKpC5TlIlMNdJqhjbuy3RwIg4kawo7h8RJ+W/mFL6dTv7vg+YmLu/2zLgFOD9+QMiYiiwKqW0GfgScEnupVuAf8+b0OAY4EsppVUR8XpEHATcS3bT9B+09yElaTvMdZIqgblOUkXaXsF7J3B87vFdwDvzXkvAdhNjSqkhd73ILUA1cElK6ZGIOA+Ym1K6ATgC+HZEpNx7fDq37aqI+AZZcgU4L6W0Kvf4U8AcoBdwc26RpJ1lrpNUCcx1kipSZJPilbf6+vo0d+7cYochqRuJiHkppfpix9GZzHWS2lJu+c5cJ6kt28p1O3pbIkmSJEmSSoIFryRJkiSpLFnwSpIkSZLK0vYmrWoSEW8CxuePTyldXqCYJKkozHWSKoG5TlIlabfgjYgrgD2B+TTfPDwBJkZJZcNcJ6kSmOskVZqOdHjrgcmpEqZzllTJzHWSKoG5TlJF6cg1vA8DIwodiCQVmblOUiUw10mqKB3p8A4FHo2IvwMbt6xMKR2/7U0kqeSY6yRVAnOdpIrSkYL364UOQpK6ga8XOwBJ6gJfL3YAktSV2i14U0p3dkUgklRM5jpJlcBcJ6nStHsNb0QcFBH3RcSaiHgjIhoj4vWuCE6Suoq5TlIlMNdJqjQdmbTqh8CpwFNAL+CjwI8KGZQkFYG5TlIlMNdJqigdKXhJKS0EqlNKjSmlS4FjCxuWJHU9c52kSmCuk1RJOjJp1bqI6AHMj4j/AJ6ng4WyJJUQc52kSmCuk1RROpLgPpgbdzawFhgDvLuQQUlSEZjrJFUCc52kitKRWZoXR0QvYGRK6d+6ICZJ6nLmOkmVwFwnqdJ0ZJbmdwLzgd/nns+IiBsKHJckdSlznaRKYK6TVGk6ckrz14EDgFcBUkrzgQkFi0iSiuPrmOsklb+vY66TVEE6UvBuSim91mpdKkQwklRE5jpJlcBcJ6midGSW5kci4v1AdURMBD4L/K2wYUlSlzPXSaoE5jpJFaUjHd7PAFOAjcBVwOvA5woYkyQVg7lOUiUw10mqKB2ZpXkd8OXcIkllyVwnqRKY6yRVmm0WvO3N2JdSOr7zw5GkrmWuk1QJzHWSKtX2OrwHA0vITne5F4guiUiSupa5TlIlMNdJqkjbK3hHAEcDpwLvB24ErkopPdIVgUlSFzHXSaoE5jpJFWmbk1allBpTSr9PKZ0OHAQsBO6IiLO7LDpJKjBznaRKYK6TVKm2O2lVRPQE3k72beB44ALgN4UPS5K6jrlOUiUw10mqRNubtOpyYCpwE/BvKaWHuywqSeoi5jpJlcBcJ6lSba/DexqwFvhH4LMRTXMbBJBSSv0LHJskdQVznaRKYK6TVJG2WfCmlLZ5fa8klQtznaRKYK6TVKlMfpIkSZKksmTBK0mSJEkqSwUteCPi2Ih4IiIWRsS5bbw+NiJuj4gHImJBRByXW/+BiJift2yOiBm51+7I7XPLa8ML+RkkqT3mOkmVwFwnqRRt97ZEuyIiqoEfkd3kfClwX0TckFJ6NG/YV4BrUkoXRsRkspkDx6eUfgH8IrefacD1KaX5edt9IKU0t1CxS1JHmeskVQJznaRSVcgO7wHAwpTSMymlN4CrgRNajUnAllkBBwDL29jPqbltJak7MtdJqgTmOkklqZAF7yhgSd7zpbl1+b4OnBYRS8m+BfxMG/t5H3BVq3WX5k57+X+RN6++JBWBuU5SJTDXSSpJxZ606lRgTkppNHAccEVENMUUEQcC61rdHP0DKaVpwKG55YNt7TgizoqIuRExd8WKFYX7BJLUPnOdpEpgrpPU7RSy4F0GjMl7Pjq3Lt9HgGsAUkp3A3XA0LzXT6HVt4AppWW5n6uBK8lOsdlKSumilFJ9Sql+2LBhu/AxJGm7zHWSKoG5TlJJKmTBex8wMSImREQPsiR3Q6sxzwFHAUTEJLLEuCL3vAp4L3nXeURETUQMzT2uBd4BPIwkFY+5TlIlMNdJKkkFm6U5pdQQEWcDtwDVwCUppUci4jxgbkrpBuCfgYsj4p/IJjo4I6WUcrs4DFiSUnomb7c9gVtySbEauBW4uFCfQZLaY66TVAnMdZJKVTTnofJVX1+f5s51tntJzSJiXkqpvthxdCZznaS2lFu+M9dJasu2cl2xJ62SJEmSJKkgLHglSZIkSWXJgleSJEmSVJYseCVJkiRJZcmCV5IkSZJUlix4JUmSJEllyYJXkiRJklSWLHglSZIkSWXJgleSJEmSVJYseCVJkiRJZcmCV5IkSZJUlix4JUmSJEllyYJXkiRJklSWLHglSZIkSWXJgleSJEmSVJYseCVJkiRJZcmCV5IkSZJUlix4JUmSJEllyYJXkiRJklSWLHglSZIkSWXJgleSJEmSVJYseCVJkiRJZcmCV5IkSZJUlix4JUmSJEllyYJXkiRJklSWLHglSZIkSWXJgleSJEmSVJYseCVJkiRJZcmCV5IkSZJUlix4JUmSJEllyYJXkiRJklSWLHglSZIkSWWpoAVvRBwbEU9ExMKIOLeN18dGxO0R8UBELIiI43Lrx0fE+oiYn1t+krfN7Ih4KLfPCyIiCvkZJKk95jpJlcBcJ6kUFazgjYhq4EfA24DJwKkRMbnVsK8A16SUZgKnAD/Oe+3plNKM3PKJvPUXAh8DJuaWYwv1GSSpPeY6SZXAXCepVBWyw3sAsDCl9ExK6Q3gauCEVmMS0D/3eACwfHs7jIiRQP+U0j0ppQRcDryrU6OWpB1jrpNUCcx1kkpSIQveUcCSvOdLc+vyfR04LSKWAjcBn8l7bULulJg7I+LQvH0ubWefAETEWRExNyLmrlixYhc+hiRtl7lOUiUw10kqScWetOpUYE5KaTRwHHBFRFQBzwNjc6fEfB64MiL6b2c/W0kpXZRSqk8p1Q8bNqzTA5ekHWCuk1QJzHWSup2aAu57GTAm7/no3Lp8HyF3rUZK6e6IqAOGppReAjbm1s+LiKeBvXPbj25nn5LUlcx1kiqBuU5SSSpkh/c+YGJETIiIHmSTF9zQasxzwFEAETEJqANWRMSw3OQIRMQeZJMYPJNSeh54PSIOys3i9yHgtwX8DJLUHnOdpEpgrpNUkgrW4U0pNUTE2cAtQDVwSUrpkYg4D5ibUroB+Gfg4oj4J7KJDs5IKaWIOAw4LyI2AZuBT6SUVuV2/SlgDtALuDm3SFJRmOskVQJznaRSFdmkeOWtvr4+zZ07t9hhSOpGImJeSqm+2HF0JnOdpLaUW74z10lqy7ZyXbEnrZIkSZIkqSAseCVJkiRJZcmCV5IkSZJUlix4JUmSJEllyYJXkiRJklSWLHglSZIkSWXJgleSJEmSVJYseCVJkiRJZcmCV5IkSZJUlix4JUmSJEllyYJXkiRJklSWLHglSZIkSWXJgleSJEmSVJYseCVJkiRJZcmCV5IkSZJUlix4JUmSJEllyYJXkiRJklSWLHglSZIkSWXJgleSJEmSVJYseCVJkiRJZcmCV5IkSZJUlix4JUmSJEllyYJXkiRJklSWLHglSZIkSWXJgleSJEmSVJYseCVJkiRJZcmCV5IkSZJUlix4JUmSJEllyYJXkiRJklSWLHglSZIkSWXJgleSJEmSVJYseCVJkiRJZamgBW9EHBsRT0TEwog4t43Xx0bE7RHxQEQsiIjjcuuPjoh5EfFQ7udb8ra5I7fP+blleCE/gyS1x1wnqRKY6ySVoppC7TgiqoEfAUcDS4H7IuKGlNKjecO+AlyTUrowIiYDNwHjgZeBd6aUlkfEVOAWYFTedh9IKc0tVOyS1FHmOkmVwFwnqVQVssN7ALAwpfRMSukN4GrghFZjEtA/93gAsBwgpfRASml5bv0jQK+I6FnAWCVpZ5nrJFUCc52kklTIgncUsCTv+VJafpsH8HXgtIhYSvYt4Gfa2M+7gftTShvz1l2aO+3l/0VEtPXmEXFWRMyNiLkrVqzY6Q8hSe0w10mqBOY6SSWp2JNWnQrMSSmNBo4DroiIppgiYgrwXeDjedt8IKU0DTg0t3ywrR2nlC5KKdWnlOqHDRtWsA8gSR1grpNUCcx1krqdQha8y4Axec9H59bl+whwDUBK6W6gDhgKEBGjgd8AH0opPb1lg5TSstzP1cCVZKfYSFKxmOskVQJznaSSVMiC9z5gYkRMiIgewCnADa3GPAccBRARk8gS44qIGAjcCJybUvrrlsERURMRWxJnLfAO4OECfgZJao+5TlIlMNdJKkkFK3hTSg3A2WQz8T1GNmvfIxFxXkQcnxv2z8DHIuJB4CrgjJRSym23F/DVVtPU9wRuiYgFwHyybxYvLtRnkKT2mOskVQJznaRSFVkeKm/19fVp7lxnu5fULCLmpZTqix1HZzLXSWpLueU7c52ktmwr1xV70ipJkiRJkgrCgleSJEmSVJYseCVJkiRJZcmCV5IkSZJUlix4JUmSJEllyYJXkiRJklSWLHglSZIkSWXJgleSJEmSVJYseCVJkiRJZcmCV5IkSZJUlix4JUmSJEllyYJXkiRJklSWLHglSZIkSWXJgleSJEmSVJYseCVJkiRJZcmCV5IkSZJUlix4JUmSJEllyYJXkiRJklSWLHglSZIkSWXJgleSJEmSVJYseCVJkiRJZcmCV5IkSZJUlix4JUmSJEllyYJXkiRJklSWLHglSZIkSWXJgleSJEmSVJYseCVJkiRJZcmCV5IkSZJUlix4JUmSJEllyYJXkiRJklSWLHglSZIkSWWpoAVvRBwbEU9ExMKIOLeN18dGxO0R8UBELIiI4/Je+1Juuyci4h86uk9J6mrmOkmVwFwnqRQVrOCNiGrgR8DbgMnAqRExudWwrwDXpJRmAqcAP85tOzn3fApwLPDjiKju4D4lqcuY6yRVAnOdpFJVyA7vAcDClNIzKaU3gKuBE1qNSUD/3OMBwPLc4xOAq1NKG1NKzwILc/vryD4lqSuZ6yRVAnOdpJJUyIJ3FLAk7/nS3Lp8XwdOi4ilwE3AZ9rZtiP7lKSuZK6TVAnMdZJKUk2R3/9UYE5K6b8i4mDgioiY2hk7joizgLNyTzdExCNtDBsAvNbG+qHAy50RRyfZVpzF2ueObNvRse2N297r23qtrfUe287dtiPju+uxHbcDY3eVua5jSvnfg7lu+0r52HZ0fFcdW+i++a7YuQ7891Dobc1121fKx7aj40sv16WUCrIABwO35D3/EvClVmMeAcbkPX8GGN56LHBLbn/t7nMbsVy0g+vnFur3spO/yzbjLNY+d2Tbjo5tb9z2Xt+R4+ux7dxtOzK+3I+tua5Tf5cl++/BXFe+x7aj47vq2Bbr+JZCrtvWa/576LxtzXXle2w7Or4Uc10hT2m+D5gYERMiogfZZAU3tBrzHHAUQERMAuqAFblxp0REz4iYAEwE/t7Bfbbl/3ZwfXdTiDh3ZZ87sm1Hx7Y3bnuvl/LxLeVj29Hx5X5szXWdp5T/PZjrtq+Uj21Hx5f7sS2FXNfea91FKf97MNdtXykf246OL7ljG7nquTA7z6aj/z5QDVySUvpWRJxHVq3fkJuJ72KgL9lEB/+SUvpDbtsvAx8GGoDPpZRu3tY+CxD33JRSfWfvV8XnsS1fxTy25jp1Nx7b8las42uuU3fjsS1vnXV8C1rwlqqIOCuldFGx41Dn89iWL4/tjvN3Vr48tuXN47tj/H2VL49teeus42vBK0mSJEkqS4W8hleSJEmSpKKx4JUkSZIklSULXkmSJElSWbLg3UER8a6IuDgifhkRxxQ7HnWeiNgjIn4WEdcVOxbtuojoExGX5f69fqDY8ZQac135MteVF3PdrjHXlS9zXXnZlVxXUQVvRFwSES9FxMOt1h8bEU9ExMKIOHd7+0gpXZ9S+hjwCeB9hYxXHddJx/aZlNJHChupdsUOHueTgOty/16P7/Jgi8hcV77MdZXBXNcx5rryZa6rDF2V6yqq4AXmAMfmr4iIauBHwNuAycCpETE5IqZFxO9aLcPzNv1Kbjt1D3PovGOr7msOHTzOwGhgSW5YYxfG2B3MwVxXruZgrqsEczDXdcQczHXlag7mukowhy7IdTW7HGYJSSndFRHjW60+AFiYUnoGICKuBk5IKX0beEfrfUREAN8Bbk4p3V/gkNVBnXFs1f3tyHEGlpIlx/lU2Jd75rryZa6rDOa6jjHXlS9zXWXoqlxXUYlxG0bR/G0BZL/MUdsZ/xngrcB7IuIThQxMu2yHjm1EDImInwAzI+JLhQ5OnWZbx/nXwLsj4kLg/4oRWDdjritf5rrKYK7rGHNd+TLXVYZOz3UV1eHtDCmlC4ALih2HOl9KaSXZNTwqAymltcCZxY6jVJnrype5rryY63aNua58mevKy67kOju8sAwYk/d8dG6dSp/HtjJ4nDvG31P58thWBo9zx/h7Kl8e28rQ6cfZghfuAyZGxISI6AGcAtxQ5JjUOTy2lcHj3DH+nsqXx7YyeJw7xt9T+fLYVoZOP84VVfBGxFXA3cA+EbE0Ij6SUmoAzgZuAR4DrkkpPVLMOLXjPLaVwePcMf6eypfHtjJ4nDvG31P58thWhq46zpFS2vVoJUmSJEnqZiqqwytJkiRJqhwWvJIkSZKksmTBK0mSJEkqSxa8kiRJkqSyZMErSZIkSSpLFrySJEmSpLJkwauKEhGHRcT9EdEQEe8pdjySVAjmOkmVwFynjrDgVaV5DjgDuLLIcUhSIZnrJFUCc53aVVPsAKRCiogPAV8AErAgpfTB3PrNRQ1MkjqRuU5SJTDXaWdY8KpsRcQU4CvAm1JKL0fE4GLHJEmdzVwnqRKY67SzPKVZ5ewtwLUppZcBUkqrihyPJBWCuU5SJTDXaadY8EqSJEmSypIFr8rZn4CTI2IIgKe+SCpT5jpJlcBcp50SKaVixyAVTEScDpwDNAIPAD8CfgMMAjYAL6SUphQvQknadeY6SZXAXKedYcErSZIkSSpLntIsSZIkSSpLFrySJEmSpLJkwStJkiRJKksWvJIkSZKksmTBK0mSJEkqSxa8kiRJkqSyZMErSZIkSSpLFrySJEmSpLL0/wGozXGTjnDgrgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1152x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting CV results\n",
    "# for each value of c2, make a plot of c1 versus train and test f1-score\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "for i, val in enumerate(params_space['c2']):\n",
    "   \n",
    "    # subplot 1/3/i\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    c2_subset = cv_results[cv_results['param_c2']==val]\n",
    "\n",
    "    plt.plot(c2_subset[\"param_c1\"], c2_subset[\"mean_test_score\"])\n",
    "    plt.plot(c2_subset[\"param_c1\"], c2_subset[\"mean_train_score\"])\n",
    "    plt.xlabel('c1')\n",
    "    plt.ylabel('Mean F-score')\n",
    "    plt.title(\"c2={0}\".format(val))\n",
    "    plt.ylim([0.80, 1])\n",
    "    plt.legend(['validation score', 'train score'], loc='upper left')\n",
    "    plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows that at very low values of c_1, the model overfits, as shown by the difference in training and test performance. Also, the test score seems to be slightly higher for c_2 = 0.1.\n",
    "\n",
    "Let's thus choose c_1 = 0.1 and c_2 = 0.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
       "    keep_tempfiles=None, max_iterations=100)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building a model with optimal hyperparams\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having tuned the model, we can now save (dump) it to a a pickle file so that we can simply import it and use later for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to a pickle file\n",
    "with open('tuned_crf_classifier.pkl', 'wb') as clf:\n",
    "    try:\n",
    "        cPickle.dump(crf, clf)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        clf.close()   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
